- [Graph Theory and Additive Combinatorics](https://yufeizhao.com/gtacbook/) 
- [优化 Milvus 性能](https://mp.weixin.qq.com/s/4gDsAF4QnmXWzomrSFRLLg)
    - Milvus 是读写分离且无状态的向量数据库，状态信息储存在 etcd 中，coordinator 节点去 etcd 请求状态并修改状态
        - 当用户需要查看状态信息、清理状态信息场景时，etcd 调试工具必不可少。
        - [BirdWatcher  是 Milvus 2.0 项目的调试工具，该工具连接 etcd 并检查 Milvus 系统的某些状态](https://mp.weixin.qq.com/s/ot-eMCKqM7aP5pEbGaMIQA)
    - Milvus 单机
        - 在单机模式下，milvus内置一个rocksdb用于代替pulsar的功能，rdb_data目录里的东西是rockdb管理的，所有insert/delete/upsert的数据都会先在rocksdb里存一份做为write ahead log，然后querynode datanode从rocksdb里把数据拉出来消费
        - 如果rocksdb里的数据消费完了，不会立刻删除，因为rocksdb有自己的gc，只不过这些数据对milvus来说已经消费过了，放着只是为了保证数据安全性，一旦milvus崩了，再启动的时候，那些没被持久化到minio里的数据还能从rocksdb里拉回来
    - 合理的预计数据量，表数目大小，QPS 参数等指标
    - 选择合适的索引类型和参数
        - 索引的选择对于向量召回的性能至关重要，Milvus 支持了 Annoy，Faiss，HNSW，DiskANN 等多种不同的索引，用户可以根据对延迟、内存使用和召回率的需求进行选择
        - 是否需要精确结果？
            - 只有 Faiss 的 Flat 索引支持精确结果，但需要注意 Flat 索引检索速度很慢，查询性能通常比其他 Milvus 支持的索引类型低两个数量级以上，因此只适合千万级数据量的小查询
        - 数据量是否能加载进内存？
            - 对于大数据量，内存不足的场景，Milvus 提供两种解决方案：
                - DiskANN
                    - DiskANN 依赖高性能的磁盘索引，借助 NVMe 磁盘缓存全量数据，在内存中只存储了量化后的数据。
                    - DiskANN 适用于对于查询 Recall 要求较高，QPS 不高的场景。
                    - DISKANN需要nvme ssd
                    - Vamana the algorithm behind the DiskANN
                      - Build a random graph.
                      - Optimize the graph, so it only connects vectors close to each other.
                      - Modify the graph by removing some short connections and adding some long-range edges to speed up the traversal of the graph.
        - 构建索引和内存资源是否充足
            - 性能优先，选择 HNSW 索引
- [Milvus 2.0 数据插入与持久化](https://mp.weixin.qq.com/s/D0xdD9mqDgxFvNY19hvDgQ)
  - 删数据逻辑
    - 调用delete删除一条已存在的数据时，它只是在某个segment里把某条数据标记为deleted，但是这个segment的数据此时仍然是一个整体，那条被删的数在内存里仍然占着空间。
    - 当你又调用insert增加一条数据时，这条新数据实际上是放入一个新的segment中，这条新数据也会占用额外内存空间。因此，随着你继续删除+insert，你会看到内存用量增加，新的这个segment执行的是暴搜，cpu用量会增加。
    - 随着新的segment中的数据达到一定量可以建索引了，indexnode就开始给这个新segment建索引，建索引就会消耗cpu。只有当某个segment中被删的数据达到20%以上，datanode开始对这个segment进行compact，
    - 把deleted的数据去除掉，剩下的数据存为一个新的segmemt。在compact之后有可能会发生小segment合并成大segment。总之，删除和更新数据会产生很多额外的工作，消耗内存消耗cpu，设计上就如此
  - How to fully delete data  https://github.com/milvus-io/milvus/discussions/24875
    - small segments are compacted to a large segment, and the small segments are marked as "soft-delete", they will be deleted by garbage collection.
    - compaction do these work: 1. merge small segments into large segment, 2. remove deleted entities from segments
    - garbage collection only does one thing: permanently delete the data files of the soft-delete segments
    - once max size of segment is defined, max rows of segment also defined, a segment with max size 512MB, if vector dimension is 128, each segment will contains no more than 1 million entities.
    - How the compaction handles a segment that contains deleted entities:
      - Let's say there is a segment that has 1000 rows and contains 10 deleted entities. 
      - Compactions firstly read the segment into memory, then pick non-deleted items to construct a new segment with 990 rows.
      - The original segment is marked as soft-delete and wait gc to clean. And index node will build a new index for the new segment.
  - Configs
    - GlobalCompactionInterval means the internal machinery triggers a compaction operation every minute "dataCoord.compaction.global.interval"
    - the SingleCompactionRatioThreshold is for this purpose. You can change this threshold to 10%, but don't set it to 0.
  - 如果用num_enrtities观察行数的话，是看不出变化的，因为num_entities不统计被删除的行数。如果你是删除之后再用query去查询主键，还能查到的话，那八成是因为你是删完就立即query，而consistency_level没有设为Strong
  - Compaction https://github.com/milvus-io/milvus/discussions/28565  https://milvus.io/blog/2022-2-21-compact.md
    - When you continually insert data into milvus, the data is not persisted into storage instantly. Querynode accumulates the data in memory to perform brute-force search on it.
    -  Datanode accumulates the data in a buffer until the buffer size exceeds a threshold. The threshold is dataCoord.segment.maxSize * dataCoord.segment.sealProportion, after the datanode flushes the buffer into storage, the data becomes a "sealed segment".
    - The value dataCoord.segment.sealProportion is 0.23, this value should not be too large or too small, the reason is:
      - only "sealed segment" can be indexed, before the "growing segment" is sealed, it always performs brute-force search and slows down the search performance. If this value is large, a large "growing segment" will lead to poor search performance.
      - If the value is small, there will be lots of small sealed segments generated, large number of small segments also slows down search performance, and brings heavy workload to datanode to compact them.
    - The major purposes of compaction:
      - merge small segments into large segments because large amount of small segments slows down search performance. Ideally, the size of each segment is near to dataCoord.segment.maxSize=512MB.
      - in milvus, delete action is "soft-delete". When you call delete(), an entity is marked as "soft-deleted" in a segment, it is invisible/ignored to search/query, but still occupies memory and storage space.
    - Milvus builds an independent index for each segment. So, if segments are compacted to a new segment, the old segments are marked as "deleted" and waiting for GC
      - and indexnode will build index for the new segment. Set indexBasedCompaction to true can avoid unnecessary index work.
- [动态 Schema](https://mp.weixin.qq.com/s/jhyePhxjUbWBicEvqxIKGQ)
  - Milvus 如何实现动态 Schema 功能
    - Milvus 通过用隐藏的元数据列的方式，来支持用户为每行数据添加不同名称和数据类型的动态字段的功能。
    - 当用户创建表并开启动态字段时，Milvus 会在表的 Schema 里创建一个名为$meta的隐藏列。JSON 是一种不依赖语言的数据格式，被现代编程语言广泛支持，因此 Milvus 隐藏的动态实际列使用 JSON 作为数据类型。
  - 动态 Schema 的 A、B 面
    - 一方面，动态 Schema 设置简便，无需复杂的配置即可开启动态 Schema；动态 Schema 可以随时适应数据模型的变化，开发者无需进行重构或调整代码。
    - 另一方面，使用动态 Schema 进行过滤搜索比固定 Schema 慢得多；在动态 Schema 上进行批量插入比较复杂，推荐用户使用行式插入接口写入动态字段数据。
- [向量数据库](https://developer.aliyun.com/article/1328709?spm=a2c6h.12883283.index.43.5ba74307ZagBs5)
    - 本质上就是给定一条向量，我们要搜索离它最近的 k 条向量，那最近的这个距离的定义可以是，比如说，内积或者欧氏距离、或者余弦距离。有了距离的定义以后，我们还要定义一些向量的搜索算法。
    - 向量搜索算法总体来说分为两类，
        - 第一类是精准搜索 KNN - FLAT，一个向量一个向量地去检索，然后取 top k，召回率会很高，但是它的执行的性能会比较差，因为要做全局扫描
        - approximate nearest neighbor，就是 ANN，那这类算法它可能回答的并不是最精准的 top k 的向量，但是这一类算法的好处是执行效率比较高。
- 海量数据相似数据查找方法
    - 高维稀疏向量和稠密向量两大方向
        - 高维稀疏向量的相似查找 - minhash, lsh(Locality-Sensitive Hashing）, simhash
            - minhash
                - 定义一个函数h：计算集合S最小的minhash值，就是在这种顺序下最先出现1的元素
                - 如果进行n次重排的话，就会有n个minhash函数，{h1(S), h2(S)…, hn(S)}, 那原来每个高维集合，就会被降到n维空间，比如S1->{h1(S1), h2(S1)…, hn(S1)}
                - 实际中因为重排比较耗时，会用若干随机哈希函数替代. 同样可以定义n个哈希函数【不需要重排，每个hash计算对应的值就行】，进行上述操作，那每个集合S就被降维到n维空间的签名。
            - LSH
                - minhash解决了高维向量间计算复杂度问题(通过minhash 机制把高维降低到n维低纬空间)
                - 但是还没解决一个问题：两两比较，时间复杂度O(n^2)
                - LSH 就是这样的机制，通过哈希机制，让相似向量尽可能出现一个桶中，而不相似的向量出现在不同的桶中. 相似度计算只在么个桶中进行，每个桶彼此之间不做相似度计算。
                - 在minhashing 签名的基础上做LSH
                    - 一个高维向量通过minhashing处理后变成n维低维向量的签名，现在把这n维签名分成b组，每组r个元素。
                    - 每组通过一个哈希函数，把这组的r个元素组成r维向量哈希到一个桶中。
                    - 每组可以使用同一个哈希函数，但是每组桶没交集，即使哈希值一样。桶名可以类似：组名+哈希值。
                    - 在一个桶中的向量才进行相似度计算，相似度计算的向量是minhash的n维向量（不是r维向量）。
            - simHash
                - Simhash技术引入到海量文本去重领域
                - google 通过Simhash把一篇文本映射成64bits的二进制串。
                    - 文档每个词有个权重。
                    - 文档每个词哈希成一个二进制串。
                    - 文档最终的签名是各个词和签名的加权和(如果该位是1则+weight，如果是0，则-weight)，再求签名[>0则变成1，反之变成0]得到一个64位二进制数。
                    - 如果两篇文档相同，则他们simhash签名汉明距离小于等于3。
                - 因为simhash本质上是局部敏感hash，所以可以使用海明距离来衡量simhash值的相似度。
                - 假设我们要寻找海明距离3以内的数值，根据抽屉原理，只要我们将整个64位的二进制串划分为4块，无论如何，匹配的两个simhash code之间至少有一块区域是完全相同的。
    - 高效的搜索算法有很多，其主要思想是通过两种方式提高搜索效率：
        - 减少向量大小——通过降维或减少表示向量值的长度。
        - 缩小搜索范围——可以通过聚类或将向量组织成基于树形、图形结构来实现，并限制搜索范围仅在最接近的簇中进行，或者通过最相似的分支进行过滤。
    - ANN 最近邻检索
        - [Comprehensive Guide To Approximate Nearest Neighbors Algorithms](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)
        - 树方法，如 KD-tree，Ball-tree，Annoy
          - KD-Tree（K维树）：KD-Tree是一种用于多维空间的数据结构
            - 在向量数据库中，KD-Tree对向量的各个维度进行划分，形成一种树状结构。每个节点都存储一个向量，并在某个维度上有一个分裂值，将数据空间分为两半
            - 对于低维数据，KD-Tree查询效率高，占用的内存相对较少。然而，随着数据维度的增加，查询效率降低，受到“维度诅咒”的影响，不适合高维数据。
          - Ball Trees：Ball Trees使用“超球体”（或简称“球”）来对数据进行分区
            - 每个节点的球内都包含数据点的子集，而子节点的球则进一步细分该空间。Ball Trees 适用于任何度量空间，特别是在高维空间中，效果可能优于KD-Tree。
            - 缺点是构建过程可能相对较慢，需要更多的内存
          - Annoy（Approximate Nearest Neighbors Oh Yeah）：Annoy是一种为大型数据集创建持久性、静态、可查询的近似最近邻索引的方法
            - 它通过构建多棵随机投影树实现。Annoy 提供了查询速度和精确度之间的良好平衡，适用于大型数据集 然而它是一个近似方法，可能不保证总是返回真正的最近邻。
            - Annoy is a tree-based indexing algorithm that uses random projections to iteratively split the hyperspace of vectors, with the final split resulting in a binary tree
            - Annoy uses two tricks to improve accuracy/recall 
              - 1) traversing down both halves of a split if the query point lies close to the dividing hyperplane, and 
              - 2) creating a forest of binary trees.
            - Annoy is still an in-memory index, the number of trees (n_trees) and the number of nodes explored (search_k), allow users to balance accuracy and speed
              - Number of Trees (n_trees): Increasing the number of trees improves search accuracy but requires more storage and computation during queries.
              - Search Effort (search_k): This parameter determines how many candidate points the system evaluates during a query. Higher values lead to better accuracy at the cost of increased query time.
            - Annoy traverses several trees in parallel to narrow down the search space when performing a query. 
              - The results from all trees are combined to approximate the nearest neighbors to the query vector.
        - 哈希方法，如 Local Sensitive Hashing (LSH)
        - 矢量量化方法，如 Product Quantization (PQ)
          - PQ将大向量空间分解为更小的子空间，并为这些子空间的每一个独立地学习一组有限的“代码簿”。向量被编码为这些子空间中最近的代码簿条目的索引
          - PQ 极大地压缩了原始向量，从而实现了存储和查询的高效率。但凡事都有双面性，由于是有损压缩，PQ 可能会损失一些精确度。
        - 近邻图方法，如 [Hierarchical Navigable Small World (HNSW)](https://towardsdatascience.com/similarity-search-part-4-hierarchical-navigable-small-world-hnsw-2aad4fe87d37)
          - HNSW利用图结构，其中每个节点都是数据中的一个向量，通过一系列层次来确保快速访问。每一层都是原始数据的一个子集，上层的数据点数量比下层少。
          - HNSW 提供了查询速度和精确度之间的良好平衡，适用于大型和高维数据集。但它需要更多的内存，构建索引的过程可能较慢。
          - M: The number of edges per element during the graph creation in each layer. A higher M value generally results in better search accuracy, but it comes at the cost of slower index-building time.
          - efConstruction: The number of neighbors considered to find the nearest neighbor at each layer during graph creation. The more neighbors considered, the better the index quality, but the slower the index building time will be.
          - efSearch: The number of nearest neighbors to consider during the vector search process. The higher the efSearch, the higher the recall, but the searching process will be slower.
          - M：NSW图中每个元素的边数。较高的M值通常会对应更好的搜索精度，但代价是更慢的索引构建时间。
          - efConstruction：构建索引时的动态候选列表大小。一般来说，候选队列越长，索引质量越好，索引构建时间也就会越长。
          - efSearch：搜索阶段的动态候选列表的大小。一般来说，efSearch越高，召回率越高，但是搜索过程会比较慢。
    - 近似最近邻 (ANN)算法
      - Voyager vs. Annoy  https://zilliz.com/learn/what-is-voyager
        - Annoy was one of the earliest libraries Spotify used for ANN search. It relies on tree-based indexing, which is effective for static datasets but requires full index rebuilds for updates.
        - Voyager addresses this limitation with dynamic indexing, making it better suited for real-time and dynamic applications
        - Voyager outperforms Annoy in terms of speed and accuracy, especially as datasets grow larger.
      - Voyager vs. hnswlib
        - Hnswlib is a well-known implementation of the HNSW algorithm. While it offers excellent performance and accuracy, 
          - it lacks production-grade features like multi-language support and fault tolerance.
        - Voyager builds on hnswlib by adding these enhancements, making it a more robust and versatile tool for real-world deployments.
      - Voyager vs. ScaNN
        - ScaNN, short for Scalable Nearest Neighbors, excels in inner-product search but does not support dynamic datasets. This limitation makes it less suitable for real-time systems that require frequent updates.
        - Voyager’s flexibility and support for evolving data make it a better choice for dynamic environments
      - Voyager vs. Faiss
        - Faiss is optimized for GPU-accelerated batch processing, making it ideal for offline tasks like training or preprocessing large datasets.
        -  Voyager is designed for real-time applications, with CPU-based operations that allow for dynamic indexing and low-latency responses.
  - [Comprehensive Guide To Approximate Nearest Neighbors Algorithms](https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)
    - 「LSH」（Locality-Sensitive Hashing）」它引入了一种哈希函数，使得相似的输入能以更高的概率映射到相同的桶中，其中桶的数量远小于输入的数量。
    - 「ANNOY（Approximate Nearest Neighbors）」它的核心数据结构是随机投影树，实际是一组二叉树，其中每个非叶子节点表示一个将输入空间分成两半的超平面，每个叶子节点存储一个数据。二叉树是独立且随机构建的，因此在某种程度上，它模仿了哈希函数。ANNOY会在所有树中迭代地搜索最接近查询的那一半，然后不断聚合结果。这个想法与 KD 树非常相关，但更具可扩展性。
    - 「HNSW（Hierarchical Navigable Small World）」它受到小世界网络思想的启发，其中大多数节点可以在很少的步骤内被任何其他节点到触达；例如社交网络的“六度分隔”理论。
      - HNSW构建这些小世界图的层次结构，其中底层结构包含实际数据。中间的层创建快捷方式以加快搜索速度。执行搜索时，HNSW从顶层的随机节点开始，导航至目标。当它无法靠近时，它会向下移动到下一层，直到到达最底层。上层中的每个移动都可能覆盖数据空间中的很长一段距离，而下层中的每个移动都可以细化搜索质量。
    - 「FAISS（facebook AI Similarity Search）」它运行的假设是：高维空间中节点之间的距离服从高斯分布，因此这些数据点之间存在着聚类点。faiss通过将向量空间划分为簇，然后在簇内使用用向量量化。faiss首先使用粗粒度量化方法来查找候选簇，然后进一步使用更精细的量化方法来查找每个簇。
      - [上手Faiss](https://mp.weixin.qq.com/s/GxxPqa1pjDvt9PvAMuebkA)
      - [Faiss: The Missing Manual](https://www.pinecone.io/learn/series/faiss/)
    - 「ScaNN（Scalable Nearest Neighbors）」的主要创新在于各向异性向量量化。它将数据点量化为一个向量，使得它们的内积与原始距离尽可能相似，而不是选择最接近的量化质心点。
      - [blog](https://blog.research.google/2020/07/announcing-scann-efficient-vector.html) [Vector Search ANN 服务](https://cloud.google.com/vertex-ai/docs/matching-engine/ann-service-overview?hl=zh-cn)
      - ScaNN 算法的核心思想是使用一种称为 Product Quantization（PQ）的技术将高维向量压缩成多个低维向量，并使用这些低维向量进行相似性搜索。PQ 技术可以将高维向量划分成多个子向量，并对每个子向量进行量化，从而将高维向量压缩成多个低维向量。这样可以大大降低相似性搜索的计算复杂度，提高搜索效率。
      - ScaNN 算法还使用了一种称为 Clustering Graph（CG）的技术，将数据集划分成多个子集，并构建一个图来表示这些子集之间的相似性关系。这样可以将相似的向量聚集在一起，从而提高搜索效率。
      - ScaNN 算法的优点在于它可以在大规模数据集上进行高效的相似性搜索，同时具有较高的搜索准确率。ScaNN 算法还支持增量式更新，可以动态地添加和删除向量，从而适应数据集的变化。
    - Faiss 
        - 我们可以将向量想象为包含在 Voronoi 单元格中 - 当引入一个新的查询向量时，首先测量其与质心 (centroids) 之间的距离，然后将搜索范围限制在该质心所在的单元格内。
        - 为了解决搜索时可能存在的遗漏问题，可以将搜索范围动态调整，例如当 nprobe = 1 时，只搜索最近的一个聚类中心，当 nprobe = 2 时，搜索最近的两个聚类中心，根据实际业务的需求调整 nprobe 的值。
    - Product Quantization (PQ)
        - 在大规模数据集中，聚类算法最大的问题在于内存占用太大
            - 保存每个向量的坐标，而每个坐标都是一个浮点数，占用的内存就已经非常大了。
            - 还需要维护聚类中心和每个向量的聚类中心索引，这也会占用大量的内存。
        - 对于第一个问题，可以通过量化 (Quantization) 的方式解决，也就是常见的有损压缩.例如在内存中可以将聚类中心里面每一个向量都用聚类中心的向量来表示，并维护一个所有向量到聚类中心的码本，这样就能大大减少内存的占用。
            - 但是在高维坐标系中，还会遇到维度灾难问题，具体来说，随着维度的增加，数据点之间的距离会呈指数级增长，这也就意味着，在高维坐标系中，需要更多的聚类中心点将数据点分成更小的簇，才能提高分类的质量。否者，向量和自己的聚类中心距离很远，会极大的降低搜索的速度和质量。
        - 对于第二个问题，将向量分解为多个子向量，然后对每个子向量独立进行量化，比如将 128 维的向量分为 8 个 16 维的向量，然后在 8 个 16 维的子向量上分别进行聚类，因为 16 维的子向量大概只需要 256 个聚类中心就能得到还不错的量化结果，所以就可以将码本的大小从 2^64 降低到 8 * 256 = 2048 个聚类中心，从而降低内存开销
    - Hierarchical Navigable Small Worlds (HNSW) 类似 skiplist
        - 这种方法的基本思想是每次将向量加到数据库中的时候，就先找到与它最相邻的向量，然后将它们连接起来，这样就构成了一个图。当需要搜索的时候，就可以从图中的某个节点开始，不断的进行最相邻搜索和最短路径计算，直到找到最相似的向量。
        - HNSW 继承了相同的分层格式，最高层具有更长的边缘（用于快速搜索），而较低层具有较短的边缘（用于准确搜索）
    - 相似性测量 (Similarity Measurement)
        - 欧几里得距离（Euclidean Distance）
            - 欧几里得距离算法的优点是可以反映向量的绝对距离，适用于需要考虑向量长度的相似性计算。
            - 例如推荐系统中，需要根据用户的历史行为来推荐相似的商品，这时就需要考虑用户的历史行为的数量，而不仅仅是用户的历史行为的相似度
        - 余弦相似度（Cosine Similarity）
            - 余弦相似度是指两个向量之间的夹角余弦值
            - 余弦相似度算法的优点是可以反映向量的方向，适用于不需要考虑向量长度的相似性计算。因此适用于高维向量的相似性计算。例如语义搜索和文档分类。
        - 点积相似度 (Dot product Similarity)
            - 点积相似度是指两个向量的点积，也就是两个向量对应位置的元素相乘之后再求和。点积相似度算法的优点是可以反映向量的绝对距离和方向，适用于需要考虑向量长度的相似性计算。例如推荐系统中，需要根据用户的历史行为来推荐相似的商品，这时就需要考虑用户的历史行为的数量，而不仅仅是用户的历史行为的相似度。
            - 点积相似度算法的缺点是需要对向量进行归一化，否则会受到向量长度的影响。例如在推荐系统中，如果用户的历史行为数量很多，那么用户的历史行为向量的长度就会很大，这样就会导致点积相似度算法的结果偏向于历史行为数量较少的用户。
            - 点积相似度算法的优点在于它简单易懂，计算速度快，并且兼顾了向量的长度和方向。它适用于许多实际场景，例如图像识别、语义搜索和文档分类等。但点积相似度算法对向量的长度敏感，因此在计算高维向量的相似性时可能会出现问题。
    - 过滤 (Filtering)
        - 在实际的业务场景中，往往不需要在整个向量数据库中进行相似性搜索，而是通过部分的业务字段进行过滤再进行查询。所以存储在数据库的向量往往还需要包含元数据，例如用户 ID、文档 ID 等信息。这样就可以在搜索的时候，根据元数据来过滤搜索结果，从而得到最终的结果。
        - 为此，向量数据库通常维护两个索引：一个是向量索引，另一个是元数据索引。然后，在进行相似性搜索本身之前或之后执行元数据过滤，但无论哪种情况下，都存在导致查询过程变慢的困难。
        - Pre-filtering：在向量搜索之前进行元数据过滤。虽然这可以帮助减少搜索空间，但也可能导致系统忽略与元数据筛选标准不匹配的相关结果。
        - Post-filtering：在向量搜索完成后进行元数据过滤。这可以确保考虑所有相关结果，在搜索完成后将不相关的结果进行筛选。
    - https://guangzhengli.com/blog/zh/vector-database/
- [向量检索技术](https://mp.weixin.qq.com/s/705usxBUmLcZL3rrB4Du9A)
  - 定义
    - 向量检索主要是做一个 K Nearest Neighbors (K最近邻，简称 KNN) 计算，目标是在N个D维的向量的库中找最相似的k个结果
    - KNN 计算通常代价比较大，很难在较短时间内返回结果，此外，在很多场景，用户并不需要绝对精确的相似结果
    - 通常会使用相似最近邻搜索，即 ANN 的方式来替代 KNN，从k个绝对最近似结果变成k个近似最优结果，以牺牲一定准确度的前提，得到更短的响应时间。
  - 向量检索的四种算法
    - Table-based，典型算法如 LSH
    - Tree-based，是把向量根据相似度去构造成一个树的结构
    - Cluster-based，也称为 IVF（Inverted File），把向量先进行聚类处理，检索时首先计算出最近的 k 个聚类中心，再在这些聚类中心中计算出最近的 k 个向量
    - Graph-based， 把向量按照相似度构建成一个图结构，检索变成一个图遍历的过程。常用算法是HNSW
  - [Vector Search Explained](https://weaviate.io/blog/vector-search-explained)
    - Vector search is a technique for finding and retrieving similar items in large datasets by comparing their vector representations, which are numerical encodings of their features
    - distance metrics, like cosine similarity and Euclidean distance (L2 distance).
    - Approximate nearest neighbors (ANN)
      - trees – e.g. ANNOY (Figure 3),
      - proximity graphs - e.g. HNSW (Figure 4),
      - HNSW 三剑客：
        - m：每本书的“直接好友”数量（影响精度 & 目录大小）。
        - ef_construct：建目录时考虑多少“邻居”（影响目录质量 & 构建速度）。
        - ef：找书时看多少“邻居”（影响搜索精度 & 速度）。
      - clustering - e.g. FAISS,
      - hashing - e.g. LSH
    - Vector indexing libraries such as FAISS, Annoy, and ScaNN
  - [Vector Search Resource Optimization Guide](https://qdrant.tech/articles/vector-search-resource-optimization/#)
    -  Overview
      - High Search Precision + Low Memory Expenditure :	On-Disk Indexing
      - Low Memory Expenditure + Fast Search Speed :	Quantization
      - High Search Precision + Fast Search Speed :	RAM Storage + Quantization
      - Balance Latency vs Throughput : 	Segment Configuration
    - Scalar Quantization
      - Qdrant compresses 32-bit floating-point values (float32) into 8-bit unsigned integers (uint8), slashing memory usage by an impressive 75%
    - Binary Quantization
      - Binary quantization takes scalar quantization to the next level by compressing each vector component into just a single bit.
    - Summary
      - 𝗕𝗶𝗻𝗮𝗿𝘆 𝗤𝘂𝗮𝗻𝘁𝗶𝘇𝗮𝘁𝗶𝗼𝗻 (𝗕𝗤)
        - • Compresses each dimension to just 1 bit (positive/negative)
        - • Memory reduction: 97%
        - • Speed: 3-4x faster than uncompressed vectors due to bitwise operations
        - • Perfect for high-volume, approximate searches
      - 𝟮. 𝗣𝗿𝗼𝗱𝘂𝗰𝘁 𝗤𝘂𝗮𝗻𝘁𝗶𝘇𝗮𝘁𝗶𝗼𝗻 (𝗣𝗤)
        - • Breaks vectors into segments and quantizes each independently
        - • Memory reduction: 85%
        - • Requires a training step to calculate centroids
        - • Trade-off: Better accuracy than BQ, approaches uncompressed performance at 97%+ recall
      - 𝟯. 𝗦𝗰𝗮𝗹𝗮𝗿 𝗤𝘂𝗮𝗻𝘁𝗶𝘇𝗮𝘁𝗶𝗼𝗻 (𝗦𝗤)
        - • Maps dimension values into 256 buckets (8 bits)
        - • Memory reduction: 75%
        - • More accurate than BQ while still offering significant compression
        - • Trade-off: Sweet spot between compression and accuracy
      - 𝟰. 𝗥𝗼𝘁𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗤𝘂𝗮𝗻𝘁𝗶𝘇𝗮𝘁𝗶𝗼𝗻
        - • Another compression technique available in modern vector databases
          - Rotate the vector by multiplying it with a rotation matrix, which is a random orthogonal matrix.
            - This step works to distribute the amount of meaningful information across all dimensions of the vector, allowing for full use of each dimension.
          - Quantize the rotated vector by reducing the number of bits used to represent each dimension of the vector.
            - This step is similar to traditional scalar quantization, but it is applied to the rotated vector.
        - • Memory reduction: 75%
        - • Focuses on preserving angular relationships between vectors
- [向量数据库](https://mp.weixin.qq.com/s/UCgJi7MfAnn8tAPvL3sldQ)
  - 向量检索算法
    - 基于树的方法，例如KDTree和Annoy
    - 基于图的方法，例如HNSW
    - 基于乘积量化的方法，例如SQ和PQ
    - 基于哈希的方法，例如LSH
    - 基于倒排索引的方法
    - 基于聚类 IVF
  - 数据压缩方式建立索引, 主要包括平坦压缩和量化压缩
    - 平坦压缩是指以未经修改的形式存储向量的索引，
    - 量化中索引的底层向量被分解成由较少字节组成的块（通常通过将浮点数转换为整数）以减少内存消耗和搜索过程中的计算成本。
  - Milvus
    - mmap功能在collection.load的时候会把s3上的数据文件下载到本地硬盘，所以首先本地硬盘得有同等大小。然后内存还是要的，最好能达到数据量的1/4以上。
    - 然后如果你用的是hnsw索引，性能可能会比纯内存慢一到两倍这样。如果用的ivf索引，性能会比纯内存慢一到两个数量级
- [Milvus]
  - ![img.png](vector_db_milvus_overview.png)
    - 接入层（Access Layer）：系统的门面，由一组无状态 proxy 组成。对外提供用户连接的 endpoint，负责验证客户端请求并合并返回结果。
    - 协调服务（Coordinator Service）：系统的大脑，负责分配任务给执行节点。协调服务共有四种角色，分别为 root coord、data coord、query coord 和 index coord。
    - 执行节点（Worker Node）：系统的四肢，负责完成协调服务下发的指令和 proxy 发起的数据操作语言（DML）命令。执行节点分为三种角色，分别为 data node、query node 和 index node。
    - 存储服务 （Storage）：系统的骨骼，负责 Milvus 数据的持久化，分为元数据存储（meta store）、消息存储（log broker）和对象存储（object storage）三个部分
      - 元数据存储：使用 etcd 作为实现，主要负责存储系统运行所必需的元信息，例如集合的 schema 定义、系统节点的状态信息以及消息消费的 checkpoint 状态等
      - 对象存储：采用 MinIO 作为默认方案，并兼容 AWS S3 和 Azure Blob 等云存储服务，用于存放日志快照、索引文件及查询过程中的临时结果
      - 消息存储：在分布式环境中采用 Pulsar，在单机版本中使用 RocksDB，作为数据流式处理与持久化的基石。
    - Milvus 的设计哲学围绕日志即数据的原则，摒弃了传统意义上的物理表结构，转而利用日志的发布订阅机制来解耦数据的读写操作，实现了系统的高度可扩展性和灵活性
    - [Segment](https://mp.weixin.qq.com/s/6TPJwoSbLio3Dai7YizGpg)
      - Think of segments as being in the intersection of shards and partitions. By design, segments cannot cross shard or partition boundaries
      - Segments are the smallest unit in Milvus for load balancing. Indexes built on separate data segments don’t depend on each other
      - Indexes are typically built only on sealed segments, posing challenges when dealing with rapidly changing data like user clicks on a shopping site
      - 数据写入：Insert 请求如何拆解为 Segment
        - proxy 会按照 shards_num 对数据进行分片，并将其发送到消息管道 ，然后data node从消息管道中异步接收数据
        - 在 data node 端，每个partition中的每个 shard 都会对应一个 growing segment。data node 为每个 growing segment 维护一个缓存，用于存放尚未落盘的数据
      - Growing Segment 如何持久化
        - 各个 shard 的数据会被累积在对应 growing segment 的缓存里。该缓存的最大容量由 milvus.yaml 中 dataNode.segment.insertBufSize 控制，默认值为 16MB
        - 一旦缓存中数据超过此阈值，data node 就会将这部分数据写入 S3/MinIO，形成一个 Chunk, Chunk 指的是 segment 中的一小段数据，并且每个字段的数据也会被分别写入独立的文件。因此，每个 Chunk 通常包含多个文件
        - dataNode.segment.syncPeriod（默认 600 秒）定义了数据在缓存中允许停留的最长时间。如果在 10 分钟内缓存数据尚未达到 insertBufSize 的上限，data node 同样会将缓存写入 S3/MinIO。
        - 当某个 growing segment 累计写入量达到一定阈值后，data node 会将其转换为 sealed segment，并新建一个新的 growing segment 来继续接收 shard 数据。
          - 这个阈值由 dataCoord.segment.maxSize（默认 1024MB）和 dataCoord.segment.sealProportion（默认 0.12）共同决定
        - 如果用户通过 Milvus SDK 调用 flush 接口，则会强制将指定 Collection 的所有 growing segment 缓存落盘并转为 sealed segment
        - 频繁调用 flush() 容易产生大量体量较小的 sealed segment，进而影响查询性能。
      - Segment 合并优化：Compaction 的三种场景
        - data node 会通过 compaction 将若干较小的 sealed segment 合并成更大的 sealed segment。合并后形成的 segment 大小会尽量接近 dataCoord.segment.maxSize（默认 1GB）
        - 小文件合并（系统自动 存在多个体积较小的 sealed segment，其总大小接近 1GB
        - 删除数据清理（系统自动） segment 中的被删除数据占比 ≥ dataCoord.compaction.single.ratio.threshold（默认 20%）
        - 按聚类键（Clustering Key）重组（手动触发） SDK 调用 compact()，并按照指定的 Clustering Key 对 Segment 进行重组。
      - 临时索引 vs 持久化索引
        - 对于每个 growing segment，query node 会在内存中为其建立临时索引; 当 query node 加载未建立索引的 sealed segment 时，也会创建临时索引
        - 当 data coordinator 监测到新的 sealed segment 生成后，会指示 index node 为其构建并持久化索引
          - 如果该 sealed segment 的数据量小于 indexCoord.segment.minSegmentNumRowsToEnableIndex（默认 1024 行），index node 将不会为其创建索引。
      - Segment 加载：Query Node 如何管理数据？
        - 当用户调用 load_collection 时：
          - (1)Query node 会从 S3/MinIO 加载该 Collection 的所有 sealed segment，并订阅相应的 shard 流数据。
          - (2)系统力求将多个 shard 分配给不同的 query node 以实现负载均衡；如果只有一个 query node，则该节点订阅全部 shard。
          - 对于每个 shard，query node 会同样在内存中维护一个对应的 growing segment，保证与 data node 上的数据保持一致。
          - 对于已构建好持久化索引的 sealed segment，query node 会从 S3/MinIO 中加载其索引文件。
      - Sharding - for distributed data writing https://zilliz.com/blog/sharding-partitioning-segments-get-most-from-your-database
        - 百万行级别数据量建议设置 shards_num=1，千万行级别数据量建议设置 shards_num=2，亿级别以上数据量建议设置 shards_num=4 或 8
        - In distributed data writes, a shard refers to a horizontal data partition in a database. 
        - Think of it like dividing a book into chapters, where each chapter resides on a separate server, and each chapter can be written concurrently.
        -  For faster writes:
          - Increase the number of shards, but not too much
          - Increase the number of data nodes if flush takes too long.
          - Add proxy servers if the network is the bottleneck
      - Partitioning - for targeted data reading
        - Imagine a book with chapters and a reader interested in a specific subject. They will read through the book based on specific criteria, akin to using a database key.
        - For targeted reading:
          - Automatic Partitioning with metadata filters if desired
          - Multi-tenancy manual use case: `tenant_id` as the partition key
            - this is just logical isolation, not real RBAC, which is defined at the collection level
  - indexnode，每个querynode，每个datanode都是单独的容器
    - indexnode只负责建索引任务，querynode只负责查询任务，datanode只负责数据的持久化和整理任务。indexnode建索引时总是会尽量用满cpu，以最快速度建好索引
      - querynode负责搜索任务，要把整个表的索引都加载到自己的内存里。
      - indexnode只负责建索引任务，你可以理解为它给segment一个一个地建索引，加载一个segment的向量进自己内存，建好索引后释放内存，再为下一个segment建索引。
      - datanode主要负责segment的落盘，和合并。合并的时候会把几个小的segment数据加载进自己内存，然后合成一个大的落盘，然后释放内存。
    - load的时候，数据会load到querynode节点上
    - 一个querynode节点search时可以用多张gpu卡同时算。但建索引的任务，一个indexnode默认是一个任务一个任务执行，所以只用到一张显卡
  - Deployment
    - standalone是内置的rocksdb实现message queue，
    - cluster是用pulsar或者kafka来做为message queue。而cdc是通过操作kafka/pulsar来同步数据的，所以standalone不能用
    - deployment推荐的resources.limits,indexnode，datanode的内存限制，refer https://milvus.io/tools/sizing/
  - Config参考 https://github.com/zilliztech/milvus-helm/blob/master/charts/milvus/values.yaml#L731C7-L731C45
    - [Milvus核心参数](https://mp.weixin.qq.com/s/FoyiQCchwY9AkOmi6ElmKA)
    - milvus rpc的传输限制，这个可以在milvus.yaml的proxy. grpc. serverMaxRecvSize/clientMaxSendSize
    - autoindex是定义在milvus.yaml里的，默认好像是hnsw索引，数据有100gb的话，索引就要100gb+的内存
    - indexbasedCompaction设为true，compaction的时候就只针对建好索引的分片，没建好索引的分片没资格参与compaction
    - bloomfiltersize应该是针对单个segment。默认是十万行。就是说如果每个分片里的行数是十万行左右的话，布隆过滤器的效果最好最适合
      - milvus内部，每个分片都有一个布隆过滤器，用于快速判断一个主键是否可能存在于该分片中。bloomfiltersize用于控制布隆过滤器的适用范围，影响布隆过滤器器的效果。
      - 布隆过滤器会被持久化，储存在每个segment的stat_log里
    - interimindex是给growing segment用的临时索引，是IVF_FLAT，参数定义在milvus.yaml的queryNode.segcore.intermindex里
    - minSegmentNumRowsToEnableIndex是给growing segment用的，是指一个segment里的行数达到多少才能建索引. 默认1024
    - dataCoord.segment.maxIdleTme是600，也就是600秒之后强制把growing转为sealed
    - GlobalCompactionInterval，这个参数是控制定期触发compaction检查的，默认是60秒
    - TTL
      - ttl feature is more like a async cleanup. if you want to trigger cleanup you will need to run manualy compaction
      - flush operate is needed to ManualCompaction success
    - PV卷（查询节点和索引节点所在容器中的/var/lib/milvus/data使用的PV卷）挂载到NVMe SSD 上就可以。values.yaml中的queryNode.disk.enabled和indexNode.disk.enabled跟queryNode.enableDisk是一样的作用，
      - 相当于是在values.yaml中加了设置磁盘索引的快捷入口，你用了values.yaml里的这个配置，就不用milvus.yaml里的配置了
    - [Milvus Kafka Topic 命名规则](https://xie.infoq.cn/article/44c20de81d705ec44c857e094)
      - 配置文件中的 chanNamePrefix
      - Milvus 会创建三种类型的 Topic：
        - Datacoord-timetick-channel——chanNamePrefix.cluster-chanNamePrefix.dataCoordTimeTick
        - Dml channel--chanNamePrefix.cluster-chanNamePrefix.rootCoordDml
        - Delta channel--chanNamePrefix.cluster-chanNamePrefix.rootCoordDelta
      - Milvus utilizes two types of channels, PChannel and VChannel. https://github.com/milvus-io/milvus/discussions/36095
        - Each PChannel corresponds to a topic for log storage --- physical channel
        - Each VChannel corresponds to a shard in a collection ----- virtual channel
        - Multiple vchannels can share one pchannel.
          - A P-channel does not always serve one collection. V-channel is assigned to a P-channel by a runtime balancer.
          - A collection with multiple vchannels, the multiple vchannels could be assigned to multiple pchannels, or assigned to a single P-channel, depending on the runtime balancer.
        - 创建collection时会指定分片(shard)数量，每个分片对应一个虚拟通道(vchannel)
        - Milvus会为log broker中的每个vchannel分配一个物理通道(pchannel)
        - 不同的collection可以共享相同的pchannel,这样可以通过log broker的高并发提高吞吐量
        - 当collection创建时,不仅指定了分片数量,同时也确定了vchannel和pchannel在log broker中的映射关系
      - Each kafka topic is a "physical channel". The number of physical channels is defined in the milvus.yaml `rootCorrd.dmlChannelNum`
      - By default, milvus creates 16 physical channels during startup
      - 修改配置
        - 如果是Docker Compose部署，可以修改配置文件 milvus.yaml，
        - 若是heml部署，可以helm upgrade my-release milvus/milvus --set queryNode.enableDisk
        - 若是 operator部署 https://github.com/zilliztech/milvus-operator/blob/main/docs/administration/configure-milvus.md
      - milVus单机默认16个channel(p/v总和)。
        - 一个collection会默认有两个channel处理TT等消息，channel的值是2*collection数量。 collection过多，会导致并发争16个channel，CPU会飙起来
  - [查询](https://milvus.io/blog/deep-dive-5-real-time-query.md)
    - milvus的调用顺序可以是：建表，insert，建索引，load，search
      - 也可以是：建表，建索引，insert，load，search
      - 还可以是：建表，建索引，load，insert，search 
      - 无论何种顺序，建索引必须在load之前，load必须在search之前
    - milvus每次查询最多只能返回16384条结果，分页也要遵循这个限制
    - milvus2.3 search和query不走消息存储, 直接由proxy发送给query node
      - proxy 中 查询，每次都要去 rootcoordi 请求一次吗？只有设置strong一致性才会每次请求rootcoord，其他一致性等级都会跳过
    - 如果你不用过滤查询的话，hnsw索引会比ivf_flat快。动态数据是要比静态数据查询慢的。对于动态数据，如果partition多的话，性能会更差一些
    - nlist取2048比较好，nprobe按你之前的比例取10左右。ivfpq，一亿的数据，nlist 可以设置成多少合适，2048吗？
      - 在milvus里，每个分片都是独立的内存，所以nlist的取值是以每个分片所包含的行数来推荐。我们推荐是4*sqrt(每个分片里的行数)。faiss都是把数据整成一份，所以它那个很大
      - nlist的推荐值是4*sqrt(n)，其中n是单个segment中的数据行数，如果milvus.yaml中的segment. maxSize为默认的512MB，则n可以用512MB/(dimension*4Bytes)来计算
      - nlist是聚类中心的数量，nprobe是搜索时查询的聚类中心的数量。nlist越大，聚类中心越多，搜索时需要查询的聚类中心也就越多，搜索的时间也就越长。
      - nprobe是搜索时查询的聚类中心的数量，nprobe越大，搜索的时间也就越长。nlist和nprobe的取值要根据实际情况来调整，一般来说，nlist和nprobe的取值要根据数据量、数据维度、索引类型、搜索参数等因素来调整。
      - nlist 和 nprobe 都是针对一个segment里的数据来说的，这两个值调控的是一个segment里面参与搜索的数据比例。nlist 是把一个segment里的数据分成多少个堆，nprobe是决定从这nlist个堆里选多少个出来参与搜索计算。
      - nlist和nprobe匹配着用就问题不大。比如nlist=1024 nprobe=128能满足你对于召回率的要求，那么把nlist改成512后重建索引，也相应地把nprobe=64，那么召回率也大概率能满足你召回率的要求。
      - 随着nlist/nprobe的较小，对召回率的影响就开始越来越大，比如16/2和8/1，这两者的召回率可能就差挺大了。反之，随着nlist/nprobe等比放大，两组召回率差别变小，但由于nlist的计算量差别增大，查询速度的差距也变大。所以才有4*sqrt(n)这么一个推荐值
    - 要明白k-means的原理，知道nlist和nprobe的关系。nlist和nprobe固定不变的情况下，被计算的数据比例基本是固定的，所以召回率也是基本不变
      - k-means是聚类出nlist中心点，nprobe是检索的时候，查询多少个中心点。随机情况下，每个中心点代表的数据条数是分片里的数据量除以nlist，那每次搜索nprobe核中心，这比例不就固定的
    - 每次search()返回的list第一个distance都是最大的？
      - 选的metric_type是L2还是IP。
      - 如果是L2的话，distance越接近0就越相似，排第一个的最相似。
      - 如果是IP的话，在向量都做了归一化的前提下，distance越接近1越相似。详情参考官网文档的metric type。
    - metric_type
      - COSINE，milvus会在建索引的时候为每一条向量计算一个模长，并把这些模长存在索引里，搜索的时候用这个模长来计算cosine的值。就相当于内部做了归一化。
      - L2是欧式距离，没必要归一化，归一化也行，但是归一化或者不归一化得出来的topk结果有可能不同.
      - IP都不会做归一化，只计算cosine公式的分子部分，如果向量是归一化的，那么分子除以分母1，结果必然和真正的cosine公式相同。 IP normalize之后就是cosine 本质上要看你关不关心模长
      - 使用IP必须要把向量都归一化，不然没有意义。对于归一化的向量，IP算出来的结果和COSINE理论上是相同的。如果你自己做了归一化，那就可以用IP。没做，那就用COSINE或者L2
      - cosine是在数据被insert进来落盘的时候就计算好向量的length，然后在查询的时候会把两条向量的内积除以它们的length，所以是计算了一个余弦。由于length已经提前算好，所以查询性能和IP相比不会差多少
    - 有向量索引的情况下，过滤查询是有可能搜不出结果。
      - 有时候能搜到有时不能搜到，多半是因为底下的segment做了compaction之后重建了索引。几个有索引的小分片和一个有索引的大分片，过滤搜索出来的东西很可能不同。
    - 查询节点内存自动均衡的几种策略？当前默认是scorebase
    - search()接口是用向量做近似搜索，必须要传入向量。query()接口应该比较符合你想查标量的功能
    - 关于query/search的结果里output_fields返回原始向量的功能
      - 从2.3.0版本开始，对于hnsw/ivf_flat索引，可以返回原始向量，直接从内存中的索引获取原始向量
      - 从2.3.2版本开始，对于ivf_sq8/ivf_pq索引，可以返回原始向量，从缓存到本地的原始向量数据文件里读取原始向量
    - milvus查看行数有两种方式，
      - 一种是collection.num_entities，只是从etcd中拿取每个segment创建时所记录的行数，不会统计delete的数量
      - 第二种是collection.query(output_fields=["count(*)"])。做一次详细统计，并把delete数量也统计在内。
        - query(output_fields="count(*)")是统计已落盘以及被querynode消费到的那部分数据。有些数据如果仍然在pulsar中就不会被统计到
      - Attu上看到的approximate count是用的前一种方式，所以不会统计delete的数量
    - Search()
      - milvus的过滤做法是先按条件里的标量过一遍，把符合条件的条目标为1，不符合的标为0，然后做ANN搜索，碰到1的就计算距离，碰到0的就忽略。过滤的性能跟索引有关系，HNSW索引如果标为1的数量很少，就很慢。IVF索引不受这个影响，比较快
    - 全量查询功能 - Query iterator
    - `collection.query(expr="id==xx", output_fields=["*"])`  这是取出一整行数据
    - 如果检索不出，一有可能是embedding不太好，二有可能索引参数搜索参数设的不合理，三有可能consistency level是eventually
      - insert数据到collection，数据的可见性跟message queue消费的速度有关，查询时想要确定数据可见就用consistency_level=Strong
      - collection创建时设置的consistency_level是做为默认值使用，不能修改
      - search和query接口都有consistency_level参数指定本次查询的level，如果没填search/query的consistency_level参数，才会使用collection的默认设定。
      - consistency_level是控制数据可见性的
        - milvus各个节点通过同步时间戳的方式来确认达到某个时间点的数据可见性，由一个rootcoord节点每隔200毫秒发送一个时间戳消息到消息中间件，如果某个时间戳被所有的querynode节点看到，那就说明这个时间戳之前的数据都已可见。
        - 设为Strong意思是等待当前最近的时间戳消息被所有querynode消费之后再开始搜索。有可能会等待200到400毫秒。
    - 向量搜索不保证你想要的那条数据一定会在结果集里的第一个，也会会在第五第六个甚至第10个 向量搜索是ANNS，“近似近邻搜索”这几个词的简写，它不叫精确搜索
    - search返回的结果里不带有partition信息。可以建表时用一个字段来存partition的名字或者标记，然后search的时候在output_fields里填写这个字段的名字。
    - 用gpu来做查询，必须要使用"GPU_"名字打头的索引
      - gpu的显存没有机器内存那么大，能加载的数据量相对较小，所以不适合巨型数据集。gpu在nq比较大的时候比较有优势，比如单次查询输入一千条以上的向量去搜
      - 每次搜索时要把目标向量从内存拷到显存，搜索完成后把结果从显存拷到内存，这些都是有成本的
      - 索引是带有GPU_前缀的索引，indexnode和querynode都需要gpu
    - qps
      - qps受影响的因素很多，数据量，维度，索引类型参数，搜索参数，是否有过滤，是否有output_fields，milvus. yaml里面的queryNode.group里的配置，querynode数量，load的参数replica_number，等等。
      - 要获得更高的qps可以从上面这些方面入手。cpu的核数和性能也会影响qps，甚至NUMA架构也会影响qps。单机版的indexnode datanode如果有建索引或者compaction的任务在执行，也会影响qps。
    - count(*)是精确值，但它得出的结果会受consistency_level的影响。如果Strong，则是完全精确的。如果是Bounded/Eventually，有可能少数数据还在pulsar里未消费完成，就不被计入行数
    - try_search_fill方法是当缓存里的结果集不多的时候，执行若干次search（搜索半径会向外扩一圈）以获取更多的结果集
      - 初始化iterator的时候有个batch_size参数，如果缓存里的结果集仍然小于batch_size，它就会再次执行search(半径继续往外扩)来获取更多的结果集
    - 多列向量查询
      - 2.4版本的多列向量查询是先分别计算每个子查询的结果，再把子结果rerank。根据topk再做reranking。不是简单的取交集，有 fusion 策略的
    - 索引中做向量相似性检索
      - 本质上都是通过预先对向量数据的分布做了一个统计，然后在搜索的时候仅计算很小的范围就能找到绝大部分结果。不管是hnsw还是ivf，搜索的时候都只计算很小一部分数据就能找到结果
      - 那如果加上过滤条件，把一大批数据都排除在计算之外，那么本来要搜索的那小范围里也会有向量被排除在外，就凑不出足够的结果
      -  localstorage对diskann有效，当你调用load的时候就会把diskann索引数据都下载到localstorage里。
      - 内存里有小部分量化过的数据，搜索的时候先在内存里做一次快速搜索，然后在localstorage里找到对应的分块文件，读出原始向量做精确检索
    - 查询延迟就主要跟querynode有关，有时querynode资源充足而proxy节点不太行的话，proxy也有可能成为瓶颈
    - milvus目前的querynode做负载均衡的时候是根据行数来估算的，如果某一个entity的text列特别大，有的特别小，balance是个挑战
    - query node cpu https://github.com/milvus-io/milvus/discussions/25571
      - a search request CPU usage is estimated by cpuRatio * nq * segment_count
      - Assume a query node has 16 cores, we say the CPU ability is 1600. The collection has 5 segments. Now you send 100 search requests at the same time with nq=8. cpuRatio=10.
        - The search engine estimates usage of each request to be 8510=400. Then it allows 1600/400=4 requests to be executed at the same time
      - maxNQ is for search requests combination. The search engine can combine similar search requests into a big request to execute.
    - [Similarity Metrics for Vector Search](https://zilliz.com/blog/similarity-metrics-for-vector-search)
      - 𝗘𝘂𝗰𝗹𝗶𝗱𝗲𝗮𝗻 𝗗𝗶𝘀𝘁𝗮𝗻𝗰𝗲 (𝗟𝟮) – for absolute distances
      -  𝗖𝗼𝘀𝗶𝗻𝗲 𝗦𝗶𝗺𝗶𝗹𝗮𝗿𝗶𝘁𝘆 – for directional comparison
      -  𝗜𝗻𝗻𝗲𝗿 𝗣𝗿𝗼𝗱𝘂𝗰𝘁 – for combined direction and magnitude
      -  𝗠𝗮𝗻𝗵𝗮𝘁𝘁𝗮𝗻 𝗗𝗶𝘀𝘁𝗮𝗻𝗰𝗲 (𝗟𝟭) – for grid-based calculations
      -  𝗛𝗮𝗺𝗺𝗶𝗻𝗴 𝗗𝗶𝘀𝘁𝗮𝗻𝗰𝗲 – for binary vector comparison
      - Choose the right metric based on your data characteristics and application needs.
        - Use Euclidean distance when you care about the difference in magnitude. It's great for when your vectors have different magnitudes and you primarily care about how far your data points are in space.
        - Use Cosine similarity when you care about the difference in orientation. Perfect for NLP applications and scenarios where vector direction matters more than magnitude.
        - Use Inner product when you care about both magnitude and orientation. It's a versatile option that works well for both normalized and non-normalized datasets.
        - Use specialized metrics like Hamming or Jaccard for binary data or specific applications where these metrics are more appropriate.
  - 索引
    - [index overview](https://www.slideshare.net/slideshow/introduction-to-multilingual-retrieval-augmented-generation-rag/267957576)
      - L2 (Euclidean) - Spatial distance 主要运用于计算机视觉领域
      - Cosine - Orientation distance
      - IP (Inner Product) - Both - 主要运用于自然语言处理（NLP）领域
        - With normalized vectors, IP is equivalent to cosine similarity
      - SQ - Scalar Quantization - Bucketize across one dimension, accracy x memory tradeoff
        - The dimensions in a vector embedding are usually represented as 32 bit floats. SQ transforms the float representation to an 8 bit integer. This is a 4x reduction in size.
        - like BQ, is a lossy compression technique. However, SQ has a much greater range.
        - SQ compressed vectors are more accurate than BQ compressed vectors
        - 把一个高精度的浮点向量，主动丢失部分精度，变为一个低精度向量，减少计算和存储开销。例如把向量中的元素 0.1192 和 0.1365 统一用 0.1 来替换
      - PQ - Product Quantization - bucketize across multiple dimensions, accuracy x memory tradeoff
        - 把高维向量空间降维。因为低维向量的存储和计算开销都远远低于高维
        - First, it reduces the number of vector dimensions to a smaller number of "segments",
        - then each segment is quantized to a smaller number of bits from the original number of bits (typically a 32-bit float).
        - PQ makes tradeoffs between recall, performance, and memory usage. This means a PQ configuration that reduces memory may also reduce recall.
      - BQ - Binary quantization - a quantization technique that converts each vector embedding to a binary representation
        - The tradeoff is that BQ is lossy. The binary representation by nature omits a significant amount of information, and as a result the distance calculation is not as accurate as the original vector embedding.
    - [向量索引选型](https://mp.weixin.qq.com/s/yJIHKfUOAtz4iWVHURri9g)
    - Index type
      - FLAT：适用于需要 100% 召回率且数据规模相对较小（百万级）的向量相似性搜索应用。
      - IVF_FLAT：基于量化的索引，适用于追求查询准确性和查询速度之间理想平衡的场景（高速查询、要求高召回率）。
      - IVF_SQ8：基于量化的索引，适用于磁盘或内存、显存资源有限的场景（高速查询、磁盘和内存资源有限、接受召回率的小幅妥协）。
      - IVF_PQ：基于量化的索引，适用于追求高查询速度、低准确性的场景（超高速查询、磁盘和内存资源有限、接受召回率的实质性妥协）。
        - m 一般设置为[4,8], nbits设置为8
      - HNSW：基于图的索引，适用于追求高查询效率的场景（高速查询、要求尽可能高的召回率、内存资源大的情景）。
      - ANNOY：基于树的索引，适用于追求高召回率的场景（低维向量空间）。
      - IVF_HNSW：基于量化和图的索引，高速查询、需要尽可能高的召回率、内存资源大的情景
    - milvus里面，每个向量字段最多只能建立一种索引，如果要换，要把旧的删除再建新的。执行search的时候总是会使用那唯一指定的索引。 查询计划无法被外界感知
    - seal compact index这几个事情有点复杂。seal之后会建一次索引，但seal的分片可能会被合并成大的分片，大的分片又要建一次索引
    - 除了DISKANN之外，所有的索引都是纯内存的。若打开了mmap，这样querynode会把数据文件下载到本地，然后通过mmap读取。内存不足的话可以考虑ivf_sq8  ivf_pw  diskindex这些索引，或者开mmap
      - mmap是对diskann索引和gpu索引以外的其他索引有效
    - 长文本一般不过滤的 mmap最合适了，标量文本后期你如果不需要使用（过滤/outputfield输出），那么可以使用partial load，不加载这个字段。如果需要使用，可以给这个字段使用mmap
    - diskann加载到内存里的是类似于一个IVF_PQ索引。搜索时是先在内存里的索引粗略地搜索一下，然后到磁盘里取出部分向量数据做更精确的搜索
    - [集群开了mmap，创建集合索引的时候是默认就开启了，还是需要collection.set_properties({'mmap.enabled': True})参数指定](https://github.com/milvus-io/milvus/discussions/33621)
      - If mmap is enabled, when you call collection.load(), the query node will download the index files to local. The default local path is configured by the localStorage.path + "/mmap" in the milvus.yaml
      - Query node maps the file between RAM and disk by mmap(). And maps the index files into memory, but the files are not immediately read from disk to physical RAM, so the memory usage is very low.
      - Query node calls unlink() to drop the index file. But the mmap file still occupies disk space.
      - When search, query node actually read data from disk, acts like read from RAM.
      - https://docs.zilliz.com/docs/use-mmap?utm_source=x
    - hnsw索引的向量类型只能用floatvector？ float16Vector可以使用和floatVector一样的索引，hnsw ivf都行
    - ivf_flat建索引的时间跟nlist的大小相关，设小点就快，大点就慢，1G的耗时1分钟也是有可能的
      - 推荐值是4*sqrt(n)，n是单个segment里的行数。4096 dim，默认单个segment 512MB，那每个segment里大约有15000条数据，那么nlist大约为4*sqrt(15000)=480，最好是转成2的比方数，那就选512。
    - Normalization
      - 归一化是指将嵌入（向量）转换为范数等于1的过程。如果内积（IP）用于计算嵌入相似性，则所有嵌入都必须标准化。归一化后，内积等于余弦相似性
    - hnsw/ivfflat所需的内存稍大于向量数据的size。ivfsq8/ivfpq所需内存大约相当于向量数据size的1/4。diskann所需内存大约相当于向量数据size的1/4到1/6。
      - HNSW 图算法的最大优点是召回率高，性能好。但是缺点也很明显，一是数据需要全部加载在内存里面里面，成本很高
      - 聚类算法的最大优点是成本低，索引性能好，但是缺点也很明显，召回率低
    - 向量在milvus里是以float32数组的形式存储和计算，每个维度是一个float32，所以每个维度占据4字节空间。500万条2048维向量，总字节数就是5M*2048*4bytes=40GB。
      - 加载到内存里所用的空间还要看是什么索引。FLAT/IVFFLAT/HNSW这些基本上是1比1，也就是占用40GB内存。IVFSQ8/IVFPQ大约是30%，也就是占用12GB内存。
      - DISKANN大约是20%左右。DISKANN索引借助磁盘，查询过程中会有读磁盘行为。其他索引都是纯内存索引
      - 如果打开mmap功能，所有的索引都可以借助磁盘来减少内存占用。
      - 比如IVFFLAT索引原本要占用40GB内存的，如果querynode没有大于40GN内存，load就会失败。但如果打开mmap之后，只需要给querynode配置更少的内存，比如配置20GB，也能加载成功查询成功，只是耗时会久一些。
    -  float16向量在有些索引类型里还是以float32来计算的，比如IVF索引。在HNSW和DISKANN里是真正以float16计算的。相应的以float16计算的索引内存占用减半，速度也快一些。
    - 使用ivf_flat，频繁插入大量数据会自动重建索引么？
      - 之前调过创建索引的接口，后面新插入的数据都会自动创建索引。每个ivf_flat索引是用当前segment的数据做训练的，前后数据分布变了不影响。
  - load
    - load是否有并发的设置呢？
      - milvus.yaml里的queryCoord.taskExecutionCap，这个设小点每批送给一个querynode加载的segment的最大数量，每个segment里有多个数据文件，querynode也有自己的并发读取的限制，跟cpu核数相关
    -  load时是加载索引列到内存还是全量字段数据到内存呢？
      - 对于向量字段来说，如果没建好索引的分片，原始向量数据会保持在内存里做搜索，建好索引的就把索引加载进内存，原先那些原始向量就会从内存释放掉。
      - 对于非向量字段，也是有索引就加载索引，没索引就把原始数据加载进内存。基本上你可以认为全量数据都在内存里。
    - 重启的时候会把之前loaded状态的表全部加载进内存。load的速度不可控，跟内部的调度和存储的读带宽相关。一般来说，千万级别的数据量load耗时分钟级的都是正常
    - 开启milvus以后，数据不加载到内存是什么问题?
      - 如果用的是 python 的 Milvus Client，create_collection 如果没输入 index param 是不会 load 到内存里的.
      - 如果没有设置这个参数，你可以单独调用create_index()和load_collection()接口
    - 把文件上传到S3上，然后把路径传给milvus让它去读取并解析。传路径的接口在pymilvus里叫utility.do_bulk_insert()
    - Query node加载数据原理 https://github.com/milvus-io/milvus/discussions/32698
      - 每个collection有一根数据管道，其中有一个querynode负责管理这根管道，我们称之为shard-leader，它从管道中接收来自pulsar的数据，数据积累在内存里，称为growing segment，
      - 当数据达到一定量，比如一百兆，就把这块数据落盘变成sealed segment，其他的querynode等加载sealed segment。
  - insert
    - [数据插入流程](https://milvus.io/blog/a-day-in-the-life-of-milvus-datum.md)
      - Proxy Nodes and the Message Queue
        - send an _insert()_ request to a proxy node. Proxy nodes are the gateway between the user and the database system,
        - A hash function is applied to the item’s primary key to determine which channel to send it to. Channels, implemented with either Pulsar or Kafka topics
      - Data Nodes, Segments, and Chunks
        - insert_log
          - collection ID -> partition ID -> Segment ID -> Field ID -> Chunk ID
      - Sealing, Merging, and Compacting Segments
    - 客户端发送一个insert请求
      - 客户端发送一个insert请求，milvus server的proxy接到请求，proxy把数据转发给pulsar/kafka，转发完之后就立刻返回，告诉客户端说insert完成
      - 数据还在kafka里，然后querynode/datanode都要向kafka订阅数据，这里就是异步的。kafka就好比milvus的WAL组件，保证插入的数据不丢
    - auto_id
      - 建表时设置主键的auto_id=true，主键就由milvus来产生，所产生的主键数值确实是往上增的，但跟那种每次+1的递增不一样
      - 自动产生的主键值是基于时间戳的，是一个很大的数字，每次insert都会用当前的时间戳来产生一批主键，同一批次里的主键是+1递增。但不同批次的主键的值有可能有间隔
    - insert_log
      - insert_log里的东西就是每个表的原始数据，包括向量数据，各个字段的数据。分片合并时，旧的分片数据不会立刻删除，要等待GC机制大约2—3小时后删除。
      - 如果停止了数据输入，等那些旧的分片都被GC清理之后，insert_log的大小就是真实的原始数据大小
      - 单个segment存放在minio里是切分成很多个小文件的，insert_log目录下依次按collection/partition/segment/field 层级，你会看到每个field目录下面有若干个小文件，那些就是binlog文件
      -  delta log是delete操作产生的文件，记录的主要是那些要被删除数据的id。delta log跟insert log无所谓什么先后，一个是delete/upsert操作产生，另一个是insert产生
      - stats log主要包含bloom flter的数据和一些统计信息，不是由外部操作产生，而是内部生成
    - milvus跟客户端之间的rpc请求，默认单次传输的数据上限是64MB.
      - 假设每行只有一条向量加一个id，向量维度是512，那么每条向量是2048字节，加上id是2056字节。64MB除以2056就是大约的行数
    - Proxy也可以做多副本的。
      - Milvus里面没有自带LoadBalancer，需要咱们自己做一个，不然扩展proxy节点还是只有一个节点有负载。这个lb 可以通过kubernetes 来实现的，可以通过service 和ingress
    - [Clearing Up Misconceptions about Data Insertion Speed in Milvus](https://zilliz.com/blog/clear-up-misconceptions-about-data-insertion-speed-in-milvus)
      - around 97% of the "Milvus insert" time observed in LangChain or LlamaIndex is spent on embedding generation, while about 3% is spent on the actual database insertion step
    - bulkinsert要求把文件上传到该milvus所使用的minio的bucket里。默认的standalone milvus会带一个minio container，它所使用的bucket name是由milvus.yaml配置，默认名是a-bucket。
      - 所以，要先把文件上传到该minio容器里a-bucket之下
  - milvus的集群热备方案，可以看下github.com/zilliztech/milvus-cdc 
    - milvus backup是先调用milvus的一个接口去得知某个collection下面有哪些segment，然后从etcd里拿到这些segment在s3上的路径，同时通过接口拿到该表的schema
    - 接着把s3上的segment 文件拷贝到备份目录里，把该表的schema存进一个json文件里也放在备份目录下。milvus是个银行，etcd是账本，s3是金库
  - milvus里主要有两种数据，一种是元数据存在etcd，另一种是数据文件存在minio (元数据存在etcd，数据文件存在minio/s3; 就好比etcd里存着账本，minio里存着钞票)
    - 数据是分片管理，主要有两种分片(segment)
      - 一种是growing segment，负责接收新插入的数据，没有索引，搜索时暴搜(在最新的版本里提供了临时索引，ivf，超过几千条数据时开始生效)
      - 另一种是sealed segment，数据是固定的，不接受新数据，每个sealed分片建立一个独立的索引，建立索引的过程就是train，ivf索引是迭代若干次得到nlist个cluster
    - 物理文件删除？这个需要数据文件（segment） 上的数据都失效才能删掉。而segment失效，要么是上面的数据全被delete，要么是被compact。
      - 不要期望物理文件在删除后立即缩小。被删除的量要达到一定量才行，比如某个segment里被删除的数据达到了10%以上，才会触发一个动作把这10%的数据真正地从磁盘上清除。
      - 清除的流程是：用那90%的数据构建一个新的segment，然后把旧的segment标记为删除，等待垃圾清理机制做最终清除。
    - milvus的文件格式是在parquet格式外面包了一层，外部工具没法直接读的
    - nfs 不能做业务存储
      - nfs没有完全实现posix 文件语义。一旦一个系统有大量的文件删除，文件重命名的请求，nfs就会出故障。nfs，只能做追加，新建的和小规模的文件删除。而且删了以后最好就是再也不管了
      - 如果把业务数据库放到nfs这种系统，只能跑跑1-2 tps这种demo。吞吐低
    - 对于standalone 的milvus来说，rdb_data里存放的数据就相当于write-ahead-log，就等同于cluster的milvus存放在pulsar里的数据
      - 所有的insert请求都会被先存入rdb_data。也就是说，你插入多少数据，rdb_data里面就有多少数据
      - rdb_data里的数据由milvus.yaml里的rockmq.retentionTimeInMinutes控制，默认是保留3天rdb_data里主要是wal，还有些time tickets message，删掉问题不大
  - Knowhere 是 Milvus 的内部核心引擎，负责向量搜索，是基于行业标准开源库（如 Faiss、DiskANN 和 hnswlib 等）的增强版本
    - Knowhere 属于开源，其部署环境更多样，可在所有主机类型上运行
    - Knowhere 依赖于 OSS 库（如 Faiss、DiskANN 和 hnswlib）
    - milvus的ivfsq8，实际用的就是faiss的IndexIvfScalarQuantizer
    - [HNSWlib](https://zilliz.com/learn/learn-hnswlib-graph-based-library-for-fast-anns)
      - HNSWlib is a library for vector search. It implements the HNSW algorithm, which creates a hierarchical graph structure for efficient similarity search in high-dimensional spaces.
  - [多租户](https://mp.weixin.qq.com/s/3QV7xjJ4G7MUxKOb0T427Q)
    - to B 大型知识库系统中，我们一般为每个租户提供一到多个 Database，以满足数据规模及知识库构建灵活度的需要
    -  to C 对话上下文的数据隔离，一般会选择在 Colletion，或 Partition Key 这两层实现。Partition Key 是逻辑上的表内隔离,  Partition Key 理解为一个 Hash 分桶的过程
      - 一个快速的选型原则是：如果你的用户数量在几千或万这个量级，你可以考虑为每个用户分配一个 Collection；如果你的用户数量在几百万甚至上千万这个量级，你应该考虑为每个用户分配一个 Partition Key。
      - 采用 partition key 方案进行数据隔离，Milvus 提供了 mmap 机制进行冷热数据向多层存储的映射。
      - Milvus 还提供了逻辑层的显式数据加载/释放能力。我们可以通过 load/release 这对操作，在业务层控制是否将一个非活跃用户的 collection 在内存上释放，或将一个回归活跃的用户的 collection 从磁盘加载至内存
      - 在 mmap 机制中，数据从磁盘到内存的移动过程应用侧是不感知的，而 load/release 在应用侧必须感知
    - [多租户实践](https://mp.weixin.qq.com/s/YpQ1TkJx9H6tawzvqstHCw)
  - attu
    -  attu里显示的approx entities number相当于用pymilvus的collection.num_entities获取行数，这个是从etcd中快速统计已落盘的行数。加载在内存里的数量有可能不一样，因为有些数据可能没落盘
  - 编译
    - 编译可以看看milvus repo下的DEVELOPMENT.md, 可以去github milvus disscusions里搜索compile关键字看别人都踩了哪些坑
    - 要编译和创建milvus镜像，参考这个帖子：https://github.com/milvus-io/milvus/discussions/31043
    - 编译milvus源码的，在运行start_standalone.sh之前，先做两件事：一是改configs/milvus.yaml里配置项，把所有带有/var/lib路径的替换为一个你机器上合法的路径，比如/tmp/milvus。
      - 二是进入development/docker/dev目录运行docker-compose up -d 以启动etcd服务和minio服务
    - 为了尽量降低环境因素的影响，可以尝试在docker里去编译：https://github.com/milvus-io/milvus/blob/master/build/README.md
  - [Cardinal 搜索引擎](https://mp.weixin.qq.com/s/4xx2U8Xyr1RetTkMtRrxyw)
    - Cardinal 是用现代 C++ 语言和实用的近似最近邻搜索（ANNS）算法构建的多线程、高效率向量搜索引擎， 将搜索引擎的性能比原来提升了 3 倍，搜索性能（QPS）是 Milvus 的 10 倍
    - 同时能够处理暴搜请求和 ANNS 索引修改请求；处理各种数据格式，包括 FP32、FP16 和 BF16;执行索引 Top-K 和索引范围搜索（Range Search）;使用内存中数据或提供基于内存、磁盘和 MMap 等不同方式的索引
    - Cardinal 利用 x86 的 AVX-512 扩展和 ARM 的 NEON 及 SVE 指令集等尖端技术，提供针对高效计算优化的代码
    - 它引入了 AUTOINDEX 机制，自动选择适合于数据集最佳的搜索策略和索引。开发者无需手动调优，能够节省时间和精力
    - 内部算法
      - 搜索算法，包括基于 IVF 和基于图的方法 
      - 帮助搜索保持所需召回率的算法，不论过滤样本的百分比如
      - 更高效的 Best-First 搜索算法迭代方
      - 定制了优先队列数据结构中的算法
  - misc
    - [Milvus 数据从 A 集群（K8S集群）迁到 B 集群（K8S集群](https://mp.weixin.qq.com/s/INvX-BVEtkchz6gFB7Ni9Q)
    - 数据管理
      - Migration made easy: Move data between platforms with Vector Transport Service (VTS), whether from Elastic/Pinecone/Qdrant or between Milvus deployments
      - Reliable backup: Use milvus-backup tool (https://milvus.io/docs/milvus_backup_overview.md) for creating snapshots on permanent storage and restore for data rollback or disaster recovery.
      - Data replication: To reach higher availability in production. You can also implement your hot-cold cluster setup with Milvus CDC (https://milvus.io/docs/milvus-cdc-overview.md).
    - 估算容量
      - 5百万128维，原始数据量大约是2.5g，工具估算时会乘以一个安全系数，这个系数一般是2到3之间，所以你看到的Loading Memory是5G多点
      - 工具是按cluster估的，每个节点都给了推荐，如果不算etcd/minio/plasar这些的话，milvus的节点的推荐内存配置大约总共27. 5g  etcd推荐3*4g，minio推荐2*8g，pulsar的比较多，因为它本身也是个分布式系统 所以如果500万128维的向量其实必要用cluster，一个standalone就好了
    - https://github.com/zilliztech/vts 迁移用这个工具
    - milvus的检索策略这里的短词和长句的权重适配策略是怎么做
      - 简单的方法就是根据用户 query 来的问题分词的长度来计算，比如长度小于3，BM25 这一路的分数给 0.7，dense 给 0.3；如果问题长度长，分数就倾向于 dense；最后根据业务召回准确率来调整这里的参数定义
    - [Milvus SDK v2 ](https://mp.weixin.qq.com/s/VXupxtWpiC1XIgbswdt3Og)
    - Milvus如何实现无感扩容自动分片
      - Milvus 利用 Kubernetes 和存算分离架构来支持无缝扩展与动态资源分配
      - 其一，基于Segment的架构：在核心上，Milvus 将数据组织成“Segment”——数据管理的最小单位：
        - Growing Segment位于StreamNodes上，优化实时查询的数据新鲜度
        - Sealed Segment 由QueryNodes管理，利用强大的索引加速搜索
        - 这些Segment在节点之间均匀分布，以优化并行处理
      - 其二，双层路由：与传统数据库中每个分片位于单个机器上不同，Milvus 将一个Shard中的数据动态分布在多个节点上：
        - 每个Shard可以存储超过 10 亿个数据点
        - 每个Shard中的Segment可以跨机器自动平衡
        - 扩展集合就像扩展Shard数量一样简单
        - 即将推出的 Milvus 3.0 将引入动态Shard拆分，甚至将这一最小的手动步骤也取消
      - 而应对大规模查询时，Milvus 整体遵循以下的过程：
        - Proxy识别请求collection的相关Shard
        - Proxy从StreamNodes和QueryNodes收集数据
        - StreamNodes 处理实时数据，而QueryNodes 同时处理历史数据。 结果被聚合并返回给用户
    - [Zilliz Cloud 建表功能升级](https://mp.weixin.qq.com/s/FuVzdn1cQCIsuML41lop0A)
      - 全面支持全文检索与文本匹配
        • 在界面中可直接启用全文检索（Term Search）和文本匹配（Term Match），无需手动组合字段与函数。
        • 用户只需选定 VARCHAR 列并配置解析器（Standard 或 Custom），便能享受自动生成稀疏向量的全文检索能力。
        • 提供可视化示例代码，便于快速上手并平滑切换到 SDK 方式。
      - Partition 与 Partition Key 概念澄清
        - • Partition 为同一 Collection 的物理子集，Partition Key 则在超多租户场景下通过标量列进行数据隔离。
        - • 全新独立分区管理页面，可创建和预览所有分区，并支持将数据直接导入指定分区。
        - • 当启用 Partition Key 时，系统会自动管理分区，避免重复或错误操作。
      - Mmap 精细化配置，释放内存潜力
        - • 允许在集合级和列级分别针对原始数据和索引数据手动开启/关闭 Mmap。
        - • 提供实时预览功能，修改只需在页面中进行 Release Collection 后即可生效。
        - • 明确了字段、集合、集群级别的 Mmap 优先级，减少配置混淆带来的性能影响。
      - 支持 Nullable 和 Default Value，界面更简洁
        - • 默认隐藏不常用的 Nullable 与 Default Value 设置，仅在需要时展开，避免干扰高频操作。
        - • 提升写入数据的容错能力，简化对不完整或动态数据的处理。
      - 完整索引管理生命周期，新增标量索引
        - • 新版建表流程支持向量索引和标量索引统一管理，并设置专门的索引管理页面。
        - • 可根据需要新建、删除、预览任意索引，支持 JSON Path 索引以加速 JSON 与动态字段查询。
      - Shard 与 Consistency Level 配置可视化
        - • Shard 池可水平拆分 Collection 以提升写入吞吐量。
        - • Consistency Level 控制跨副本最新数据的可见性，界面提供默认配置解释与修改入口。
        - • 让用户在建表时便可根据业务需求进行调优，减少后期改动成本。
      - Dynamic Field 展示优化
        • 对动态字段与向量/标量列进行并列展示，说明它是“特殊列”并介绍适用场景。
        • 适用于数据结构不固定且需要频繁新增列的场景，可随时插入新字段而无需改变 Schema。
  - [优化](https://zilliz.com.cn/blog/milvus-%20community-keyword)
    - 降低内存
      - 牺牲性能 - Mmap, DiskANN
      - 牺牲精度 - IVF_PQ, IVF_SQ8(基于Faiss)
    - 分批插入数据，是每一批 collection.flush()，还是最后再 collection.flush()？
      - 插入数据时，不要每一批都调用 flush()接口，Milvus 内部会定期调 flush() 接口，所有数据都插入完成之后，再调用一次 flush 即可
      - 插入数据后，频繁调用 flush，会产生大量碎的 segment 文件，为系统带来较大的 compaction 压力
    - 这个设置成主键之后，为什么还可以继续重复插入相同的值？
      - Milvus 用 insert 接口做数据插入时，不会做主键去重，如果希望主键去重，可以使用 upsert 接口
      - 但是，由于 upsert 内部多做了一次 query 操作，插入性能会比 insert 更差
      - 将 Upsert 替代 Insert 使用，这类操作往往会导致系统收到大量不存在主键的 Delete 请求。
      - 在 Milvus 中，处理 Delete 操作时通常会先借助 BloomFilter 判断主键是否存在，从而避免浪费资源在为不存在的数据进行 Delete 操作
      - 在百万级 segment 场景下，BloomFilter 带来的内存消耗是巨大的
      -  Milvus 2.5.8 中我们做出调整：DataNode 不再将 BloomFilter 加载至内存，所有 Delete 记录直接持久化到底层对象存储。
      - 即便 Delete 的主键在当前数据中不存在，也不再进行过滤，而是交由后续阶段统一处理 最终一致性由 Compaction 机制保证
    - Support partial upsert feature https://github.com/milvus-io/milvus/issues/29735
    - 半数使用问题是配置问题
      - 怎么调？ https://mp.weixin.qq.com/s?__biz=MzUzMDI5OTA5NQ==&mid=2247496778&idx=1&sn=018256ae415356e3ed357ee473dc1627&chksm=fa5155f2cd26dce4095b57fa4eb5c7e67e9eb49ed2762ce618e1134bb33022049e75c25e49ab&scene=21#wechat_redirect
      - Milvus 从 2.3.0 版本开始，就开始支持动态调整参数，具体操作方法参考：https://milvus.io/docs/dynamic_config.md
    - [Performance Evaluation and Monitoring Metrics](https://zilliz.com/learn/how-to-spot-search-performance-bottleneck-in-vector-databases)
      - important evaluation metrics are Recall, Latency, and Queries Per Second (QPS).
      - Note
        - Grafana's Minimum Intervals Can Impact Performance Monitoring Results - Change Min interval to 15s
        - A NUMA-Design Machine Can Affect Milvus’s Performance
      - The proxy can become the bottleneck if QPS is high and performs network-intensive tasks
        - you can simply scale the Proxy vertically (use more CPU/Memory on the host) or horizontally (add more Proxy pods with a Load Balancer at the front).
      - Performance is bounded by the CPU usage of QueryNode
        - An excessive number of QueryNodes can detract from performance due to the increased number of messages the delegator must handle.
      - IndexNote reaches 100% CPU usage
        - If possible, use bulk insert rather than insert vectors one by one. This reduces network transmissions and skips the process of "growing" the segments.
        - Ignore growing segments; pass ignore_growing in the search parameters like in https://milvus.io/docs/search.md.
        - Change consistency levels to Eventually in the search API call.
    - [定位Milvus性能瓶颈](https://mp.weixin.qq.com/s/0q9PJi_pdOZKvjg_VrTivw)
      - 客户端是性能瓶颈，请考虑增加请求的数量
        - 检查并调整可能限制数据流的网络限制器。
        - 如果您使用 PyMilvus：
          - a.在多进程操作中，选择使用 spawn 方法而非 fork
          - b.在每一个进程中，执行以下操作：导入 from pymilvus import connections，然后运行 connections.connect(args)。
        -  进行水平扩展——增加客户端数量，直至 QPS 值稳定
      - 带宽
        - 启用压缩 Milvus 各组件间以及 Milvus 与 SDK 客户端间实施 gRPC 压缩
      - 磁盘性能
        - 推荐使用 NVMe SSD。在 SATA SSD 或 HDD 上创建和搜索磁盘索引可能会因为 I/O 操作受限而导致较大的 Latency 和较低的 QPS
        - 虽然 EBS 采用 NVMe 驱动，但其提供的低延迟优势并不及本地 NVMe SSD。
    - [如何优化 Milvus 性能](https://mp.weixin.qq.com/s/4gDsAF4QnmXWzomrSFRLLg)
      - Milvus 会创建 256 个消息队列 topic。如果表数目比较少，可以调整 rootCoord.dmlChannelNum 减少 topic 数目降低消息队列负载
      - 每个 collection 会使用 2 个消息队列 topic（shard），如果写入非常大或者数据量极大，需要调整 collection 的 shard 数目
        - 建议每个 shard 写入/删除不超过 10M/s，单个 shard 的数据量不大于 1B 向量，shard 数目过大也会影响写入性能，因此不建议单表超过 8 个 shard。
      - 使用多副本可以扩展查询性能，但建议副本数目不要超过 10 个
      - 索引的选择
        - 是否需要精确结果
          - 只有 Faiss 的 Flat 索引支持精确结果, 查询性能通常比其他 Milvus 支持的索引类型低两个数量级以上，因此只适合千万级数据量的小查询
        - 数据量是否能加载进内存？
          - [DiskANN](https://milvus.io/blog/diskann-explained.md)
            - DiskANN 依赖高性能的磁盘索引，借助 NVMe 磁盘缓存全量数据，在内存中只存储了量化后的数据。
            - DiskANN 适用于对于查询 Recall 要求较高，QPS 不高的场景。
            - 关键参数 search_list: search_list 越大，recall 越高而性能越差
            - DiskANN 算法首先提出了新的图结构 Vamana，Vamana 图与 HNSW、NSG 图类似
            - 为了规避多次随机读写磁盘数据，DiskANN 算法结合两类算法：聚类压缩算法和图结构算法
              - 一是通过压缩原始数据，仅将压缩后的码表信息和中心点映射信息放到内存中，而原始数据和构建好的图结构数据存放到磁盘中，只需在查询匹配到特定节点时到磁盘中读取
              - 二是修改向量数据和图结构的排列方式，将数据点与其邻居节点并排存放，这种方式使得一次磁盘操作即可完成节点的向量数据、邻接节点等信息的读取
            -  Vamana, the core data structure behind DiskANN
              - Vamana's key concept is the relative neighborhood graph (RNG)
              - the key concept is that RNGs are constructed so that only a subset of the most relevant nearest edges are added for any single point in the graph
              - There are two main problems with RNGs that make them still too inefficient for vector search. 
                - The first is that constructing an RNG is prohibitively expensive  n^2
                - The second is that setting the diameter of an RNG is difficult
          - IVF_PQ
            - 对于精确度要求不高的场景或者性能要求极高的场景。
            - [IVF 参数](https://mp.weixin.qq.com/s/BSPGfBNA2gw2_Wud8JobQA)
              - nlist：一般建议 nlist = 4*sqrt(N)，对于 Milvus 而言，一个 Segment 默认是 512M 数据，对于 128dim 向量而言，一个 segment 包含 100w 数据，因此最佳 nlist 在 1000 左右。
              - nprobe：nprobe 可以 Search 时调整搜索的数据量，nprobe 越大，recall 越高，但性能越差
            - PQ 参数
              - m：向量做 PQ 的分段数目，一般建议设置为向量维数的 1/4，M 取值越小内存占用越小，查询速度越快，精度也变得更加低
              - nbits： 每段量化器占用的 bit 数目，默认为 8，不建议调整
        -  构建索引和内存资源是否充足
          - 性能优先，选择 HNSW 索引
            - HNSW 参数
              - M：表示在建表期间每个向量的边数目，M 越大，内存消耗越高，在高维度的数据集下查询性能会越好。通常建议设置在 8-32 之间。
              - ef_construction：控制索引时间和索引准确度，ef_construction 越大构建索引越长，但查询精度越高。要注意 ef_construction 提高并不能无限增加索引的质量，常见的 ef_constructio n 参数为 128。
              - ef: 控制搜索精确度和搜索性能，注意 ef 必须大于 K。
          - 资源优先，选择 IVF_FLAT 或者 IVF_SQ8 索引
            - IVF 索引在 Milvus 分片之后也能拿到比较不错的召回率，其内存占用和建索引速度相比 HNSW 都要低很多
            - IVF_SQ8 相比 IVF，将向量数据从 float32 转换为了 int8，可以减少 4 倍的内存用量，但对召回率有较大影响，如果要求 95% 以上的召回精度不建议使用。
      - 合理选择流式插入和批量导入
        - 尽可能批量写入，整体吞吐会更高，建议每次写入的大小控制在 10M
        - 单个 Shard 的流式写入量不建议超过 10M/s
        - Datanode 多于 Shard 的情况下，部分 DataNode 可能无法获得负载
        - 导入目前支持的文件大小上限是 1GB，接下来会支持更大的导入文件大小上限
        - 不建议频繁导入小文件，会给 compaction 带来比较大的压力
      - 谨慎使用标量过滤，删除特性等特性
        - Milvus 使用的是前过滤，即先做标量过滤生成 Bitset，在向量检索的过程中基于 Bitset 去除掉不满足条件的 entity。
        - 对于 HNSW 这一类的图索引而言，标量过滤并不会加速查询，反而可能导致性能变差。
      - 常见的参数调整
        - Segment 大小：Segment 大小越大，查询性能越好，构建索引越慢，负载越不容易均衡。Milvus 默认选择 512M Segment 大小主要是考虑到了内存比较少的机型。对于内存在 8G-16G 的用户，建议 Segment 大小调整到 1024M，16G 以上的机型可以调整到 2G。
        - Segment seal portion: 当 Growing Segment 达到 Segment 大小 * seal portion 后，流式数据就会被转换为批数据。通常情况下建议 Growing segment 的大小控制在 100-200M 左右，调小这个值有助于降低流式写入场景下的查询延迟。
        - DataNode Segment SyncPeriod: Milvus 会定时将数据 Sync 到对象存储，Sync 越频繁故障恢复速度越快，但过于频繁的 sync 会导致 Milvus 生产大量小文件，给对象存储造成较大压力。
    - [如何找到精度与性能的黄金点](https://mp.weixin.qq.com/s/qUxqfr0KA9lwhwtNCZ9maw)
      - Zilliz Cloud 发布最新功能：level 参数调节搜索精度和 enable_recall_calculation 返回精度预估
      - Recall 和 Latency/QPS 的需求大不同
        - 推荐系统：召回精度要求低，高 QPS 低延迟
        - 人脸识别：召回精度要求高，低 QPS 延迟宽松
      - Level 参数调节召回精度
        - 较低的 level 参数 (level=1)：适用于一般场景（召回率通常在 90% 以上），此时查询性能优异，资源消耗较低，非常适合需要快速响应的业务需求。
        - 较高的 level 参数 (level=10)：针对对召回率要求极高的场景，提升搜索精度，但会消耗更多的计算资源和时间，适用于对精准度要求严苛的任务。
    - [How to Filter Efficiently Without Killing Recall](https://milvus.io/blog/how-to-filter-efficiently-without-killing-recall.md)
      - Graph Index Optimization: 
        - Preserves paths between similar items even when filters remove connecting nodes, preventing the “islands” problem that reduces result quality.
        - We implemented the Alpha strategy alongside other optimization techniques. 
          - implementing a dynamic combination of “include-all” and “exclude-all” traversal methods that intelligently adapts based on data statistics to optimize query performance
        - strike a balance between recall and performance
      - Metadata-Aware Indexing: 
        - Creates specialized paths for common filter conditions, making filtered searches significantly faster without sacrificing accuracy.
        - the main graph index is called the base graph, while the specialized graphs built for specific metadata fields are called column graphs.
      - Iterative Filtering: 
        - Processes results in batches, applying complex filters only to the most promising candidates instead of the entire dataset.
      - AutoIndex: 
        - Uses machine learning to automatically tune search parameters based on your data and queries, balancing speed and accuracy without manual configuration.
  - Milvus 2.3
    - Cosine 相似度类型： 无需向量归一化，简化数据搜索流程。
    - Upsert 数据：提升更新和删除数据的管理流程效率，适用于频繁更新数据且追求数据一致性和原子性的场景。
    - 范围搜索（Range Search）: 通过限制查询向量与其他向量之间的距离，范围搜索能够实现对搜索结果的有效细化，适用于搭建推荐引擎的场景。 
    - 支持 Parquet 文件：提升数据处理能力，支持 Parquet 文件，通过其高效的列式存储格式，提供更好的查询性能，适用于具有复杂数据集的场景。 
    - 支持 Array 数据类型：支持在搜索过程中基于多个属性进行精确的元数据过滤。在电商领域中，该功能支持根据不同产品标签进行搜索，为用户返回相关的搜索结果
  - Milvus 2.4
    - 支持 CAGRA 索引: CAGRA 是 NVIDIA RAFT 库中最先进的基于图形处理器的图形索引; CAGRA 即使在小批量查询中也表现出压倒性的优势
    - 还支持了 GPU 暴搜，性能有数十倍提升，进一步满足需要高召回率的场景。
    - 支持多向量搜索: 在 Collection 中存储和搜索多个向量列
    - Grouping 搜索: 用户可以在搜索 vector 的基础上做分组聚合，返回的 TopK 是基于分组后的聚合结果而非简单的以向量为中心的片段信息
    - 支持稀疏向量（beta）: 专为由 SPLADEv2 等神经模型和 BM25 等统计模型生成的向量设计
    - 倒排索引和模糊匹配支持: 
      - 基于内存的二进制搜索索引和 Marisa Trie 索引用于标量字段索引。然而，这些方法是内存密集型的
      - 2.4 采用了基于 Tantivy 的倒排索引，它可以应用于所有数字和字符串数据类型。此版本还支持模糊匹配标量过滤使用前缀，中缀和后缀
    - 内存映射存储: 不是将文件内容直接加载到内存中，而是将文件内容映射到内存中
    - 元数据过滤中支持使用正则表达式对子字符串进行匹配、全新的标量倒排索引（由 Tantivy 贡献）以及用于检测并同步 Milvus Collection 中数据变化的 Change Data Capture 工具
    - HNSW 还没有全链路支持 float16，底层 knn search的时候，是把 float16转换成 float32 再运算的，所以你看到内存使用量下降不多.下个版本，HNSW 全链路都支持 float16 之后，预期内存使用量会下降一半
    - 全链路支持float16和bfloat16的只有diskann。下一步支持的是hnsw，ivf和scann还排在后面
  - Milvus 2.4.3
    - Metadata Filtering in Milvus v2.4.3
      - you can match strings using prefix, infix, postfix, or even character wildcard searches.
      - all the variations are possible as well as using array values, either by exact matches or by checking if any elements in the array match (contains_any())
      - https://github.com/milvus-io/bootcamp/blob/master/bootcamp/Retrieval/imdb_metadata_filtering.ipynb
  - Milvus 2.5
    - [Sparse BM25](https://milvus.io/blog/full-text-search-in-milvus-what-is-under-the-hood.md)
      - 向量里的值，都是根据文档长度 |D|， TF(q) : 单词 q 在文档中的词频 来生成的，这些都是局部信息
      - avgdl : 平均文档长度， N : 文档总数，这些全局信息都是存在meta里的，不是在具体的向量值里，也不是在每个segment里。
      - 做向量查询的时候，计算分数需要计算完整的BM25公式，同时查询过来的时候也需要多花一点时间做计算（包括从 meta 取 avgdl 等全局信息的时间）。
      - [具体功能：](https://mp.weixin.qq.com/s/ikOFS19LT7DZrctDwBUOkg)
        - 分词和数据预处理：基于开源搜索库 Tantivy 实现，包括词干提取、词形还原和停用词过滤等功能。
        - 分布式词表和词频统计：高效支持大规模语料的词频管理与计算。
        - 稀疏向量生成与相似度计算：通过语料库的词频(Corpus TF)构建稀疏向量，并基于查询词频(Query TF)和全局逆文档频率 (IDF) 构建查询稀疏向量，再通过特定的 BM25 距离函数进行相似度计算。
        - 倒排索引支持：实现基于 WAND 算法的倒排索引，同时 Block-Max WAND 算法和图索引的支持也在开发中。
          - WAND的核心思想是利用打分上界（Upper Bound Score）来提前跳过不会进入Top-K的文档，做到高效剪枝
  - Milvus 2.6
    - [IVF_RABITQ 索引](https://mp.weixin.qq.com/s/jx2Isz0zfcERri3EdXc6jg)
      - 融合了 RaBitQ、IVF 倒排索引、随机旋转变换（Random Rotation）以及后处理优化机制（refinement），在高效压缩和高精度检索之间取得更优平衡。
      - 关于 IVF、RaBitQ 以及精调过程（refinement）过程的一些底层配置参数说明：
        - nlist 和 nprobe 是所有基于 IVF 方法的标准参数：
          - nlist：一个非负整数，表示整个数据集被划分成的 IVF 分桶（bucket）数量。
          - nprobe：一个非负整数，表示在搜索过程中为每个查询向量访问的 IVF 分桶数量。 属于与搜索相关的参数。
        - rbq_bits_query 指定查询向量的量化级别：
          - 可选值为 1 到 8，对应 SQ1 到 SQ8 的量化等级。
          - 设置为 0 时表示不对查询向量进行量化。 属于搜索相关参数。
        - refine、refine_type 和 refine_k 是用于精排过程的标准参数：
          - refine：布尔值，表示是否启用精排策略。
          - refine_k：一个非负浮点数，表示精排时所选候选池的放大倍数。系统将从一个大小为 refine_k 倍的候选集合中，使用更高精度的量化方式选出最终的近邻结果。属于搜索相关参数。
          - refine_type：字符串，指定用于精排阶段的量化类型。可选值包括 SQ6、SQ8、FP16、BF16 和 FP32 / FLAT（无压缩的原始精度）。
    - [重写 Milvus 的流处理架构 woodpecker](https://mp.weixin.qq.com/s/3a1TbQSSOxAsuJyjYmaSSw)
      - WoodPecker 面向对象存储优化的 WAL 引擎
      - 采用 “ZeroDisk” 架构，所有日志数据存储于云对象存储（如 Amazon S3、GCS、阿里 OSS），元数据则由 etcd 等分布式 KV 系统管理。
      - 架构
        - Client：用于提交读写请求的接口层
        - LogStore：处理写缓冲、异步写入与日志压缩
        - Storage Backend：后端存储（支持 S3、GCS、EFS 等）
        - ETCD：管理元数据并协同分布式节点
      - 两种部署模式：
        - MemoryBuffer 模式（轻量且免维护）
        - QuorumBuffer 模式（低延迟 & 高容错）
    - 除了 Woodpecker，Milvus 2.6 还引入了 Streaming Service —— 一个专用于日志接入、增量写入与实时订阅的核心组件。
      - 它取代了 Kafka/Pulsar 在数据链路中的角色，成为真正意义上的实时数据流通引擎。
    - Woodpecker is the storage layer that handles the actual persistence of write-ahead logs, providing durability and reliability
    - StreamingService is the service layer that manages log operations and provides real-time data streaming capabilities
    - [MinHash LSH ，低成本去重百亿文档](https://mp.weixin.qq.com/s/LKa5eznjqlUDzCwp4GnGqg)
      - MinHash 就是通过压缩原始集合为固定长度的签名向量，近似估算相似度，从而大幅降低计算开销
        - 先将每个文档拆分成由单词或字符构成的短语切片集合，然后将每个文档的切片集合应用多个哈希函数，并记录每个函数作用后得到的最小哈希值，为每个文档生成一个签名向量。
        - 在计算相似度时，两个文档的 MinHash 签名中相同位置的哈希值一致的概率，可以很好地拟合原始切片集合之间的 Jaccard 相似度
      - 即使使用了紧凑的 MinHash 签名，在数百万甚至数十亿个文档中进行两两比对，计算成本依然非常高。此时就需要引入 局部敏感哈希LSH方法（Locality Sensitive Hashing,）
        - MinHash LSH的核心思想是：
          - 把每个 MinHash 签名划分为 多个带（band），然后将相似条带使用标准哈希函数将其哈希到某个桶中，如果两个文档的任意一个条带落入同一个桶，即被标记为可能相似
          - 高度相似的文档往往有很多相同的哈希值，因此至少在某一带中落入同一个桶的概率更高
      - 我们可以通过调整带数和每带的维度数 ，在召回率（找到所有相似对）、精确率（减少误报）和速度之间进行权衡。
      - MinHash + LSH（局部敏感哈希） 提供了一种兼顾效率与效果的近似去重策略，适用于百亿级语料下的预处理优化；
      - MinHash 通过将文档压缩为签名，LSH 高效缩小搜索空间，可以快速定位潜在重复对
      - https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md
      - 𝗠𝗶𝗻𝗛𝗮𝘀𝗵 𝗟𝗦𝗛 𝘂𝘀𝗲𝘀 𝗮 𝘁𝘄𝗼-𝘀𝘁𝗲𝗽 𝗽𝗿𝗼𝗰𝗲𝘀𝘀:
        - 𝗠𝗶𝗻𝗛𝗮𝘀𝗵 𝗙𝗶𝗻𝗴𝗲𝗿𝗽𝗿𝗶𝗻𝘁𝗶𝗻𝗴: Converts documents into compact signatures using word sequences and hash functions.
        - 𝗟𝗼𝗰𝗮𝗹𝗶𝘁𝘆-𝗦𝗲𝗻𝘀𝗶𝘁𝗶𝘃𝗲 𝗛𝗮𝘀𝗵𝗶𝗻𝗴(𝗟𝗦𝗛): Groups similar documents into buckets, transforming exponential comparisons into linear operations.
    - [2.6功能预览](https://mp.weixin.qq.com/s/UnvVbKSjTsyz8HzRTbs2gw)
      - https://www.youtube.com/watch?v=Wb3jPzfx97Y&list=PLPg7_faNDlT4UvZtZ5GIZb8YGG_1tfKp-
      - 降本提速：
        - 引入RabitQ量化兼顾内存和召回率；一个二进制量化方法
          - 一种新的量化算法，用于向量压缩，提供高搜索精度（比传统乘积量化高 3-4% 的召回率，比标量量化高 10%）
          - 针对硬件进行了优化（SIMD 指令），在相似召回率下，QPS（每秒查询数）比原始 IVF 索引翻倍，并节省内存.
        - Sparse-BM25性能提升，QPS最高提7倍；
          - 设计了灵活的近似检索策略（如 drop_ratio_search 与 dim_max_score_ratio），让用户在精度与速度之间灵活调控
          - 在工程实现中引入了 SIMD 加速、数据预取机制等，引入并优化了 Block-Max WAND 与 Block-Max MaxScore 等高性能剪枝算法
        - JSON Path Index加速动态字段过滤，含Json field的过滤搜索延迟大幅下降。
          - 用于加速对动态字段内部特定路径下数据的过滤操作
        - Milvus Storage V2 (存储 V2)：重新设计的存储层，充分利用 Parquet 和 Arrow 格式，使其与现有数据栈兼容
          - 它通过将大字段和小字段拆分到不同的行组/文件，引入了对更多数据类型（如长文本和 Blob 存储，例如图像、音频）的支持
          - 向量存储在 Parquet 之外，以避免昂贵的序列化; 这使得点查询的性能提高了 50 倍
      - 结构体列表/嵌入列表数据模型 (Struct List / Embedding List Data Model)：
        - 一种新的数据模型，其中主键可以代表文档 ID，每行可以包含嵌入列表（例如，文档的多个块）或结构化列表（例如，嵌套元数据）
      - 搜索与功能增强：
        - 增强Analyzer/Tokenizer功能，支持多语言；
          - 新增 Run Analyzer 语法支持，提供分词配置的可观测性
          - 新增 Lindera 分词器，支持日语、韩语等亚洲语种
          - 新增 ICU 分词器，适合多语言场景
        - 引入Phrase Match功能精准匹配短语词序；支持基于短语结构和词序的精确匹配。
          - 由于不同 tokenizer 对 position 的定义方式不同，相同短语在不同分词器中可能产生不同的 slop 值。建议在使用 Phrase Match 前，通过 run_analyzer 先行验证分词效果。
        - Decay Function实现时间衰减重排序；支持第三方模型集成，简化工作流。
          - 许多信息检索和推荐场景中，内容的时效性是一个至关重要的因素。用户往往更关注最新的资讯、最近发生的事件或近期活跃的项目。
          - 传统的相似性搜索可能仅仅依据内容本身的匹配度进行排序，而忽略了时间维度对信息价值的影响。
          - Decay Function 允许用户在获取初步的搜索候选集之后，根据每个条目的时间戳信息，对其原始相关性得分进行调整。
        - 查询抽样 (Query Sampling)：允许从集合中抽取数据以评估搜索召回率和准确性，无需预先存在的基础事实数据集
      - 架构优化：
        -  引入Tiered Storage数据冷热分层，平衡性能与成本；
           - 这是一项标志性功能，允许您将热数据和冷数据分离，并在对象存储之上提供一个缓存层
           - 对于长时间未访问的冷数据，系统可基于 LRU 算法主动卸载，以腾出内存资源。
           - 同时保持热数据性能与 HNSW 或 DISKANN 类si
           - 主要特性包括延迟加载（Lazy Load）、部分加载（Partial Load）和基于 LRU 的缓存逐出（LRU-based Cache Eviction），实现“先元后数、按需拉取、自动回收”的高效流程。
        - Streaming Service增强实时向量处理能力；
        - 支持100k Collection；
        - 采用Woodpecker云原生日志系统；
        - 优化File format v2和Coord Merge 。
        - N-gram 索引 (N-gram Index)：通过将数据拆分为更小的 token（例如 2-gram、3-gram）并对其构建索引来加速文本匹配和正则表达式
        - Minhash：一种局部敏感哈希索引，专门用于大规模数据去重（例如 100 亿个向量），实现相似性去重而非精确哈希比较。这有助于识别和删除近似重复的内容
  - Milvus 3.0
    - [Support Streaming Service in Milvus](https://github.com/milvus-io/milvus/issues/33285)
      - 零磁盘架构（Zero-Disk Architecture） https://zhuanlan.zhihu.com/p/15809814733
  - Error Check list
    - "deny to write: memory limit exceeded" 意思是某个querynode或者datanode的内存快用光了
    - "unrecognized dtype for key: labels"  这是因为langchain.MilvusVectorStore没法根据你在Document的metadata中的"labels"这个key所对应的vlaue推断出这是个什么类型的字段
    - "MilvusException: (code=65535, message=efConstruction out of range" HNSW索引的参数设的有问题，要设在区间里。milvus以segment为单位管理数据。同一个collection的segments有可能被放在不同的querynode里
    -  表加载不起来，要用一个debug工具来release。具体步骤：
      - 1. 下载这个repo到本地 github.com/milvus-io/birdwatcher
      - 2. 进入repo目录，命令行执行  go build -o birdwatcher main.go ，前提是安装了go，执行成功后在該目录下会有一个birdwatcher的可执行程序
      - 3. 命令行运行 bridwatcher，进入commandline模式
      - 4. 执行 connect，前提是你的milvus所使用的那个etcd在运行状态，并且容器端口2379暴露出来
      - 5. 如果connect成功，再执行force-release
      - 6. 重启milvus容器
    - "channel not available" 报错可能是已知问题, 先release/load看一下行不行，不行的话重启一下querycoord/querynode
    - minio里面noSuchKey？
      - no suck key一般是在2.3早期的版本偶现。原因是某些bug导致gc把东西误删，导致load失败。主要有两种情况，一种是segment的bloomfilter被误删，另一种是segment的数据文件被误删。
      - 前一种情况可以通过手动调用compact就能重建bloomfilter。后一种情况相当于数据丢了回不来了，只能用birdwatcher的segment drop命令把被误删的segment信息从etcd里清除。
    - request limit exceeded[limit=1024] - 调整proxy.maxTaskNum这个参数
    -  some node(s) haven't received input
      - 这种报错有可能是collection或者partition的数量太多了，datanode在处理timetick消息的过程中超时。要么减少表和分区的数量，要么把milvus.yaml里的watchTimeoutInterval改高点。
    - failed to search: out of range in json: ef(56) should be larger than k(200)
      - 这个报错意思是你给search设置的topk值是200，hnsw的ef参数要大于topk，但你给ef设值是56。ef的值要大于topk的值，设计上如此
    -  'Timeout was reached' when loading collection https://github.com/milvus-io/milvus/discussions/41043
      - Try increasing the timeout value minio.requestTimeoutMs: Or decrease the common.threadCoreCoefficient.middlePriority to lower the concurrence of loading chunk files
  - QA
    - [𝗛𝗼𝘄 𝗠𝗶𝗹𝘃𝘂𝘀 𝗦𝗼𝗹𝘃𝗲𝘀 𝘁𝗵𝗲 𝗦𝗰𝗮𝗹𝗮𝗯𝗶𝗹𝗶𝘁𝘆 𝗣𝗿𝗼𝗯𝗹𝗲𝗺](https://milvus.io/blog/why-manual-sharding-is-a-bad-idea-for-vector-databases-and-how-to-fix-it.md) 
      - Milvus takes a fundamentally different approach, enabling seamless scaling from millions to billions of vectors without the complexity:
      - 𝗔𝘂𝘁𝗼𝗺𝗮𝘁𝗲𝗱 𝗦𝗰𝗮𝗹𝗶𝗻𝗴 𝗪𝗶𝘁𝗵𝗼𝘂𝘁 𝗧𝗲𝗰𝗵 𝗗𝗲𝗯𝘁: Kubernetes + disaggregated storage-compute architecture
      - 𝗦𝗲𝗴𝗺𝗲𝗻𝘁-𝗕𝗮𝘀𝗲𝗱 𝗔𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲: Growing segments on StreamNodes for real-time data, sealed segments on QueryNodes with powerful indexes
      - 𝗧𝘄𝗼-𝗟𝗮𝘆𝗲𝗿 𝗥𝗼𝘂𝘁𝗶𝗻𝗴: Each shard stores 1+ billion data points, with segments automatically balanced across machines
      - 𝗘𝗳𝗳𝗼𝗿𝘁𝗹𝗲𝘀𝘀 𝗘𝘅𝗽𝗮𝗻𝘀𝗶𝗼𝗻: Adding capacity is as simple as increasing shard count — no manual intervention required
    - [De duplication of the same vector](https://github.com/milvus-io/milvus/issues/5607)
    - milvus创建一个collection后，还能再添加动态字段吗？dynamic schema 打开之后，就可以再加列
    - 为何不用float64来保证小数点后十几位？
      - 一来因为float32计算起来比float64快得多，也省内存。
      - 二来向量搜索简称ANNS，本身就是近似搜索，小数点后十几位的值没有意义，就好比我们比较两个人是否长得相似不会去一根根计算他们的头发数量是否相等。
      - milvus处理向量是以float32位来传输、存储以及计算，你就算输入的是float64的值，传输到server那边就已经变成float32
    - 如何查看日志？
      -  https://github.com/milvus-io/milvus/tree/master/deployments/export-log 这里有脚本可以看日志 
    - 日志里面待构建索引行数和已构建索引行数之和大于数据总行数是什么原因呢？而且这个pendingIndexRows的数量还会增加是怎么回事呢？ 索引的已构建行数最大是等于表的总行数
      - pendingrow增加是因为里面有compaction，有些segment已经建好了索引，但它要和其他segment合并成更大的segment，合并完之后又会再次建索引
    -  milvus 如何做到不停止服务的情况下，全量重建索引
      - milvus里有一个alias的功能，比较适合这种全量更新的场景
    - 按照标量查询数据，同样的表达式，为什么一个有结果一个没结果？
      - 表达式过滤搜索时，如果带有索引比如ivf_flat索引，查出的结果可能有时有有时没有，因为内部是先过滤再做ann search。你换成FLAT就有稳定结果。
    - milvus如果做成一项公共向量服务，每个系统都是按需load/release索引的话，在数据量几百万/几千万这个级别的时候，搬运一次其实挺慢的，要好几秒的时间，那milvus的查询不就变得很慢了么，有没有什么好的建议?
      - 加载起来 每次load release本身就比较慢。如果成本考虑 不在乎查询性能 试试mmap
      - 请问开了mmap功能，collection的load方法就不用调用了，还是调用了load和release也没效果？
        - mmap 和 load/release 逻辑无关，目前 milvus 里的 collection 还是需要 load 才能进行操作
      - 请问这个load collection，是异步的把索引也一并载入到内存么？
        - 一个表里的数据划分为多个segment，每个segment都有单独的索引。对于有索引的segment，加载它的索引到内存里。对于没有索引的segment，加载原始的向量数据进内存
        - load()调用的时候，如果目前有数据不在内存里，它会阻塞模式等待这些数据进入内存。一旦load()执行完成返回，后续如果有新的数据或者新的索引，新的东西将会被异步地加载进内存。
        - 对于之前以原始数据加载进内存的segment，一旦它的索引构建完成，这个索引将会被加载进内存，然后之前在内存里的原始数据会被free，也就是一个内存中的替换。
      - 开了 mmap 即便超出内存一般也不会 oom，但是会影响 latency，我们测下来 hnsw 可以放 4 倍左右性能还能保持不错的水位，这个和你的 workload 有关。
        - diskann和mmap其实都挺依赖磁盘性能。mmap如果内存太少的话性能也会很差
        - mmap开启以后，内存资源可以降低到之前的1/4左右
        - mmap开了确实会减少内存使用。不开的话理论值是25—30gb，sizing tool会乘以一个安全系数（一般是2，有的索引可能系数大一点）
      - 另一种节省内存的方式是使用ivf_sq8或者ivf_pq，但是召回率可能不太好
    - 测试了一下milvus的100W hnsw查询和1000W hnsw查询  单次时延几乎无区别 milvus的1000W qps急剧下降， 并发不高 就30-60个并发 每个nq=1？
      - milvus有一个合并查询的机制，当你有大量并发查询，并且每个查询请求的nq都不大，并且查询参数相似时，后台会把多个请求合并成一个大的查询来执行。这样做是为了提高QPS，但对于每个请求来说就拉长了延时。
      - 在milvus.yaml的queryNode.grouping.enabled可以设为false关闭。默认是true
      - 如果跑分 segment调大应该性能就会变好
    - Error: incomplete query result - topk小于一定值是正常的，大于一定值不正常
      - 这里面确实是有bug，不过不好复现。上次另一个用户说设置topk大于某一个值时就不报错了，他设置的topk是五百多
    - milvus单机版迁移到集群的方案（含数据迁移）
      - milvus-backup项目是做milvus数据的备份恢复的，用这个工具把单机的数据备份出来，然后把备份目录上传到集群所使用的s3 bucket里，最后再用milvus-backup工具恢复数据到该集群。
      - milvus-backup check命令检查下配置
    - [milvus-backup 工具](https://mp.weixin.qq.com/s/BWbwgBlhNajFOi2df0UaKg)
      - 在同一个 Milvus 实例内进行备份和恢复：在同一 Milvus 实例内将 Collection 复制为一个新的 Collection。
      - 在共用同一个 S3 Bucket 的两个 Milvus 实例之间进行备份和恢复：在使用不同根路径但使用相同 S3 Bucket 的 Milvus 实例之间迁移 Collection。
      - 在同一个 S3 服务不同 Bucket 的两个 Milvus 实例之间进行备份和恢复：在同一 S3 服务内的不同 S3 Bucket 之间迁移 Collection。
      - 在不同 S3 服务的两个 Milvus 实例之间进行备份和恢复：在使用不同 S3 服务的 Milvus 实例之间复制 Collection。
    - datanode写minio的速度跟不上从pulsar读取数据的速度，导致datanode的内存不断升高直到oom，怎么能控制datanode读取pulsar的速度呢
      - 一般可以通过quota反压 或者datanode加资源解决
    - 需要一个clip模型帮你做embedding，milvus只是做向量搜索。milvus是否支持通过大模型输文字到milvus里面搜索对应的图片的功能？
    - milvus有没有关于向量数据的插入性能测试，先创建索引在插入？
      - 插入的负载主要会在proxy，datanode和MQ，如果是先建索引和load再做插入的话，indexnode建索引的快慢也会影响growing segment的数量，进而影响datanode的负载然后影响插入的性能；我们测试过插入吞吐200+MB/s的场景，理论上还可以更高
    - 搜索结果不全，有可能是以下两种情况：
      - 一种是刚insert的数据不一定能搜出来，把search的consistency_level参数设为Strong，能保证该时间点之前的数据都能搜出来，但latency会增加几百毫秒。
      - 另一种是带索引加过滤条件，因为索引只搜一小块数据，加了过滤之后，那一小块里面有可能被过滤掉很多，导致结果不足。你把索引换成FLAT，也就是暴搜，就可以搜到。
    - [Milvus源码开发](https://mp.weixin.qq.com/s/kKmcVciEf_WcuiGwYzYMxg)
    - [How does Milvus GPU work](https://github.com/milvus-io/milvus/discussions/36328)
      - milvus loads GPU index into GPU RAM. If the GPU RAM is insufficient, you will get an error about load failure.
      - When you call search(), the request's vector is copied into GPU RAM to compute, then result is copied from GPU RAM to CPU RAM, and returned to the client. So, there is an extra cost for copying data between GPU RAM and CPU RAM.
      - GPU is faster than CPU for large number of parallel tasks. So, GPU index is suitable for large NQ search, NQ is the number of vectors you input to search().
    - [the difference between SPARSE_INVERTED_INDEX and SPARSE_WAND](https://github.com/milvus-io/milvus/discussions/34806)
      - SPARSE_INVERTED_INDEX is the plain old inverted index: for each dimension, maintain a list of doc ids that is not zero in this dimension; at search time, only compute scores of docs in merged lists of the query's non zero dimensions.
      - SPARSE_WAND is similar to SPARSE_INVERTED_INDEX in that they both have the same inverted index, but SPARSE_WAND maintains more metadata in the index to allow the utilization of the WAND algorithm to greatly reduce the number of docs that needs to be evaluated. 
      - The choice between SPARSE_WAND and SPARSE_INVERTED_INDEX mostly depends on the density(average number of non zeros / total number of possible dimensions) of the dataset and the queries. 
        - In most cases SPARSE_WAND should be much faster than SPARSE_INVERTED_INDEX, especially for doc embeddings with higher density and datasets with tons of docs. 
        - But SPARSE_WAND's performance downgrades fast as the dimensions of the query increases: every time when we see slower WAND, we see long queries with small drop_ratio_search.
      - The default value for drop_ratio_build/search is 0. With this setting both indexes yield 100% recall rate. And the same drop_ratio_build/search yields the same recall rate on SPARSE_WAND/SPARSE_INVERTED_INDEX.
      - suggest to start from drop_ratio_build=0.3 and drop_ratio_search=0.6 and see if that meets your recall requirement, and adjust as needed. 
      - Also I suggest to adjust drop_ratio_search more aggressively if it's ok to sacrifice recall a little for a huge performance improvement
      - https://weaviate.io/blog/blockmax-wand
    - 在milvus中一个collection 刚插入数据的时候，这个数据可以检索到，但是过几分钟就查不出来了，我查询条件是从1000万数据里面取top200
      - 在 Milvus 中插入新数据后，立即进行搜索，可以检索到新插入的数据。这是因为新数据尚未被纳入索引，搜索过程对新数据采用的是精确的暴力搜索（Brute-Force Search）
      - 经过一段时间，Milvus 后台会自动触发索引构建或合并过程，新数据被纳入向量索引中。此时，搜索过程主要依赖于索引结构进行近似搜索，可能导致无法检索到先前插入的数据。
    - 数据分布不均匀
      - 先部署个监控和webui 看看有几个segment 是不是在三台机器上均匀 数据太小了三台用不起来 需要开三replica.Milvus是分segment的 
      - 一百万数据可能就一两个segment 加再多机器也只有一两个有数据 replica可以解决这个问题
      - 1024维，150万数据是6GB，大约会有七八个segment。如果有集群里有3个querynode，并且每个querynode的cpu资源跟你的standalone的cpu资源相等，那么150万数据的这个集群查询性能会比50万数据的这个单机稍慢一些，
        - 因为集群内部几个节点间有通信，3个querynode的结果集还要做合并。
    - [使用外部pulsar](https://github.com/milvus-io/milvus/discussions/40914)
      - 除了milvus.yaml中的mq.type设置成pulsar之外，还要milvus.yaml中的pulsar.address/port这些设置正确
      - 这个pulsar最好能允许milvus创建topic，如果不能创建，那就得设置milvus.yaml中的common.preCreatedTopic指定你预先设置的topic
- [BigANN 2023](https://mp.weixin.qq.com/s/7H7xtGzEfAdu-zQv0NHYzg)
  - Filters 赛道: 本赛道使用了 YFCC 100M 数据集，要求参赛者处理从该数据集中选取的 1000 万张图片
    - 具体任务要求为提取每张图片的特征并使用 CLIP 生成 Embedding 向量，且需包含图像描述、相机型号、拍摄年份和国家等元素的标签（元素均来自于词汇表）。
    - 主要挑战是参赛者需要成功地将 10 万个查询与数据集中的相应图像和标签匹配，每个查询都包含一个图像向量和特定的标签。
    - 解决方案是基于图算法和 tag 分类
      - Build 时，会对每种可能出现的 tag 组合进行 cardinality 分析；对于较包含向量较多的组合建图，其他的组合建立倒排索引；搜索时则根据不同的组合选取对应的搜索方式
      - 对 query 按所属的 tag 进行分类，搜索时分别对每个 tag 所对应的 query 依次进行搜索
        - 一是可以最大化 cache 利用率，二是在爆搜时可以使用矩阵乘法计算进行加速。当然，为了加速计算，我们使用了对数据进行量化，并利用 SIMD 优化相关的距离计算
  - Out-Of-Distribution(OOD) 赛道: 本赛道使用了 Yandex Text-to-Image 10M 数据集，强调跨模态数据的整合。
    - 基础数据集包括来自 Yandex 视觉搜索数据库的 1000 万张图像 Embedding 向量，这些向量由 Se-ResNext-101 模型生成的，但查询向量是基于文本搜索和不同的模型生成的
    - 参赛者需要有效地弥合这些不同模态数据之间的差异
    - 解决方案基于图算法以及高度优化的搜索过程
      - 计算方面，用不同精度的量化进行搜索和 refine，并使用 SIMD 进行计算加速
      - 搜索开始前，先对 query 向量进行聚类。在图搜时，对分属不同的聚类的 query 分配不同的初始点，并且对每个聚类进行依次搜索。
        - 聚类可以体现两大优势，一是对不同聚类依次搜索优化了 cache 利用率，二是由于对不同聚类分配自适应的初始点一定程度上缓解了向量不同分布的问题
        - 使用了多级的 bitset 数据结构
          - 在图搜的过程中，一般需要一个数据结构来标记哪些点被访问过，传统的方法是使用一个 bitset 或者哈希表，bitset 的缺点是其中大部分的内存并不会被使用到，使得每次读取时会有大概率的 cache miss
          - 哈希表的缺点是糟糕的常数导致性能低下
          - 受内存中多级页表的启发，设计出了多级 bitset 的数据结构，将上层 bitset 缓存在 cpu cache 中，使得读写性能得到大幅提升。
  - Sparse 赛道: 本赛道以 MSMARCO passage retrieval 数据集为核心，处理大量的文章（超过 880 万个），所有文章都通过 SPLADE 模型编码成稀疏向量。
    - 查询数量近 7000个，且查询也通过相同的模型处理，但长度较短、非零元素较少。
    - 本赛道的主要挑战是选手需准确检索给定查询的 Top 结果，重点是查询向量和数据库向量之间的最大内积。
    - 解决方案是基于图算法和一些基于稀疏向量的优化
      - 每个稀疏向量通过 (data[float32], index[int32]) 元组的列表进行表示，对 data 进行多精度量化，分别用户图搜时的计算和最后的 refine。同时 index 也可以用 int16 表示，以降低内存带宽使用。
      - 内积计算有一个重要特性即绝对值较大的值有更大的重要性，而绝对值较小的值有较小的重要性。
      - 可以根据这一特性设计剪枝策略，即在图搜过程中将绝对值较小的值裁剪掉，并在图搜结束后使用完整向量进行 refine。
      - 还用 SIMD 进行快速的有序列表求交集，进而实现高效的稀疏向量内积计算
  - Streaming 赛道: 本赛道基于 MS Turing 数据集中的一部分，包含 1000 万个数据
    - 参与者的任务是遵循提供的“操作手册”（该手册详细说明了一系列数据插入、删除和搜索操作），并在 1 小时内完成操作。此外，选手们还有 8GB DRAM 的限制
    - 赛道的重点是优化处理这些操作的过程，并保持数据集的索引流畅。
    - 解决方案是基于图算法以及 SQ 量化
      - 对于删除操作， 采用了惰性删除策略
      - 搜索中，对向量进行不同精度的量化分别用以图搜和 refine
        - 第一步是使用低精度量化后的向量进行图搜。
        - 由于使用惰性删除策略，搜索得到的一些向量已经被删除了，因此在第二步使用后置过滤的策略将被删除的向量过滤掉
        - 用更高精度的量化向量进行 refine 得到最终结果。
  - https://github.com/harsha-simhadri/big-ann-benchmarks/pulls?q=Zilliz+Solution+author%3Ahhy3
- [向量检索大赛](https://mp.weixin.qq.com/s/pngwV1Ibe4rxmtWrcr_D2g)
- [Vector DB Comparison](https://www.superlinked.com/vector-db-comparison)
  - Pinecone vs Weaviate
    - Pinecone Key Features
      - Compute and Storage Separation:
      - Static Sharding: By employing static sharding within its POD-based clusters, Pinecone ensures seamless distribution of data for enhanced query processing
    - Weaviate
      - Contextualized Embeddings: Weaviate's unique ability to generate contextualized embeddings enhances the understanding of data relationships
      - hybrid search strategy combines dense vectors for contextual understanding with sparse vectors for keyword matching
- 稠密、稀疏和二进制 embedding 向量，它们各自的优势和劣势
  - Sparse vectors are very high-dimensional but contain few non-zero values, making them suitable for traditional information retrieval use cases
    - Typically (but not always), the dimensions represent different tokens in one or more languages, with values assigned to each, indicating their relative importance in that document.
    - This type of layout is beneficial for tasks that require some keyword matching.
    - BM25 is a well-known algorithm for generating sparse vectors, improving upon TF-IDF by incorporating a saturation function for term frequency and a length normalization factor.
    - 稀疏向量能够高效表示高维但特征活跃度较低的数据。这种表示方式在文本处理中尤为常见，因为一篇文档通常只使用词汇表中的一小部分词汇
  - Dense vectors, on the other hand, are embeddings from neural networks that, when combined in an ordered array, capture the semantics of the input text
    - These vectors are typically generated by text embedding models and are characterized by most or all elements being non-zero
    - This makes them highly effective for semantic search, as they return the most similar results based on distance, even without exact keyword matches.
  - Summary
    - dense embeddings are better for encoding the semantics or fuzzy meaning of a piece of text
      - dense embedding -> L2 or cosine
      - Use L2 distance because it effectively captures the overall similarity in a continuous vector space, 
      - which aligns with the semantic similarity encoded in dense embeddings.
    - sparse embeddings are better for encoding exact or adjacent concepts
      - sparse embedding -> IP
      - Use Inner Product because it measures the overlap in non-zero dimensions, 
      - which is crucial for capturing exact or adjacent concepts in high-dimensional sparse vectors.
  - [ binary embeddings](https://zilliz.com/learn/what-are-binary-vector-embedding)
    - Binary embeddings are a type of vector representation in which each dimension is encoded using a single binary digit, typically represented as either 0 or 1.
    - Binary embeddings offer storage efficiency and computational speed, limited memory resources or large datasets
    - may not capture all the nuances or complexities in the original data.
    - How to Generate Binary Embeddings
      - Hashing-based methods utilize locality-sensitive hashing (LSH) or random projections to directly map high-dimensional input data to binary codes
      - Deep learning architectures, such as deep belief networks (DBNs) and restricted Boltzmann machines (RBMs), generate binary embeddings as part of their output
      - Quantization-based techniques convert continuous-valued embeddings into binary representations.
    - https://weaviate.io/blog/binary-quantization
    - 针对计算机视觉、图像指纹以及推荐系统等场景设计，适用于数据以二进制特征表示的情况
    - 在需要精确匹配的场景（如图像去重、数字水印和版权检测）中，经过优化的二值索引能提供精准的相似性检测
    - 而在对速度要求高于完美召回的场景中（如高吞吐量的推荐系统和大规模特征匹配），二值索引也能展现出优异的性能。
  - • 𝗦𝗽𝗮𝗿𝘀𝗲 𝗘𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀: https://weaviate.io/learn/knowledgecards/sparse-embeddings?utm_source=channels&utm_medium=w_social&utm_campaign=dev_education&utm_content=knowledge_cards_680689769
  - • 𝗗𝗲𝗻𝘀𝗲 𝗘𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀: https://weaviate.io/learn/knowledgecards/dense-embeddings?utm_source=channels&utm_medium=w_social&utm_campaign=dev_education&utm_content=knowledge_cards_680996000
  - • 𝗕𝗶𝗻𝗮𝗿𝘆 𝗘𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀: https://weaviate.io/learn/knowledgecards/binary-embeddings?utm_source=channels&utm_medium=w_social&utm_campaign=dev_education&utm_content=knowledge_cards_680640507
  - • 𝗤𝘂𝗮𝗻𝘁𝗶𝘇𝗲𝗱 𝗘𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀: https://weaviate.io/learn/knowledgecards/quantized-embeddings?utm_source=channels&utm_medium=w_social&utm_campaign=dev_education&utm_content=knowledge_cards_680040494
  - • 𝗩𝗮𝗿𝗶𝗮𝗯𝗹𝗲 𝗗𝗶𝗺𝗲𝗻𝘀𝗶𝗼𝗻 𝗘𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀: https://weaviate.io/learn/knowledgecards/variable-dimensions?utm_source=channels&utm_medium=w_social&utm_campaign=dev_education&utm_content=knowledge_cards_680791951
    - 可以只使用前8、16、32等维度，同时保留大部分信息。这种能力来自模型训练：前面的维度比后面的维度捕获更多信息
  - • 𝗠𝘂𝗹𝘁𝗶-𝗩𝗲𝗰𝘁𝗼𝗿 𝗘𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀: https://weaviate.io/learn/knowledgecards/multivector-embeddings?utm_source=channels&utm_medium=w_social&utm_campaign=dev_education&utm_content=knowledge_cards_680980442
    - 多向量Embedding 不是每个对象一个向量，而是获得代表对象不同部分的多个向量（如文本的token，图像的patch）。这实现了"延迟交互" - 比较文本的各个部分而不是整个文档
  - 𝗪𝗵𝗶𝗰𝗵 𝘁𝘆𝗽𝗲 𝘀𝗵𝗼𝘂𝗹𝗱 𝘆𝗼𝘂 𝘂𝘀𝗲? It depends on your use case:
    - • Need semantic matching? → Dense embeddings
    - • Storage-constrained? → Binary or quantized
    - • Complex documents? → Multi-vector
    - • Keyword search? → Sparse vectors
  - [Vector Visualization](https://milvus.io/docs/vector_visualization.md)
  - [Embedding Models for Audio Data](https://zilliz.com/learn/top-10-most-used-embedding-models-for-audio-data)
- VectorDB challenge
  - 工程
    - 向量索引的 合并、拆分 - small index merge into large index
    - 向量索引的 高效更新 - mutable vs immutable index
  - 采用HNSW + DiskANN的混合检索策略
    - HNSW delta + DiskANN
- ByteHouse 主要在向量检索计算下推、过滤操作优化、数据冷读问题优化几个方面采取了优化措施
  - 计算下推
    - 首先，对每个 Part 进行 Vector Search，相当于将一个算子拆分成三个算子，先做Vector Search。
    - 然后，对 Vector Search 的结果进行全局排序，此时不读取标量信息列。
    - 最后，在全局排序的结果上，执行 Read Task，得到最终结果
  - 过滤操作优化
    - 基于标量主键范围查找
      - 标量其实是一个排序键，类似于 MySQL 中的主键。通常，在进行搜索时，会先进行标量过滤，从而获取符合查询条件的数据。如果按一般方法计算，就会有很大的消耗。
      - 由于标量本身是有序的，所以可简单理解为：只需读取首尾部分数据，进行过滤，构建符合条件的row id bitmap。比如在计算timestamp大于某个值的情况时，只需计算开始和结束位置所对应的行，因为中间结果都是符合的。
    - 加速标量列剪枝
      - 数据剪枝，主要是根据实际情况对物理上的键排列分区，包括对主键和辅助索引的分区，以此来加速查询。
    - 存储层过滤
      - 在存储层面进行优化，将过滤条件下推到存储中，尽量减少 IO 操作。对类似OLAP和OLTP的数据库而言，查询动作的底层会有很高的计算开销
- [Vector Database vs Graph Database](https://zilliz.com/learn/vector-database-vs-graph-database)
- [Weaviate]
  - [ACORN (ANN Constraint-Optimized Retrieval Network)  filter search](https://weaviate.io/blog/speed-up-filtered-vector-search)
    - improves speed and accuracy when using rule-based filtered with vector-based search by employing predicate subgraph traversal
    - benefit those of you with very large datasets
    - improve performance challenges with negatively correlated filtered HNSW searches
    - Filtered search has two main drawbacks:
      - The filter can start at the “wrong” end of the vector space, making it slow.
      - The filter can also filter out the most relevant items to the search query. This is called negatively correlated filtered search.
    - pre-filter or post-filter?
      - pre-filtering determine which objects match the filter, skip the HNSW graph index, and just brute force these filtered objects
      - post-filtering we first search for the closest vectors to our query using the HNSW graph index, and then we apply the filter.
      - use the flatSearchCutOff param to trigger switching between the two solutions. The post-filtering solution, however, is enhanced by filtering the result set inline while searching.
    - The Weaviate implementation of ACORN differs from that of the paper in a few ways
      - The first important change is during building the graph. While it is true that a different pruning logic helps keep the graph connected, we decided not to modify the indexing from “vanilla” HNSW.
      - The second important difference is how we explore the graph while querying. Weaviate's ACORN implementation conditionally evaluates whether to use the two-hop expansion.
      - The third difference is how we seed additional entry points at layer zero of the graph to better deal with the problem of queries having low correlation to the filter.
- [𝗠𝗨𝗩𝗘𝗥𝗔: converting multi-vector embeddings into single fixed-size vectors](https://weaviate.io/blog/muvera)
  - 𝗦𝗽𝗮𝗰𝗲 𝗣𝗮𝗿𝘁𝗶𝘁𝗶𝗼𝗻𝗶𝗻𝗴
    - Divides the vector space into "buckets" using techniques like SimHash or k-means clustering. Each vector gets assigned to a bucket, creating sub-vectors for each partition.
  - 𝗗𝗶𝗺𝗲𝗻𝘀𝗶𝗼𝗻𝗮𝗹𝗶𝘁𝘆 𝗥𝗲𝗱𝘂𝗰𝘁𝗶𝗼𝗻
    - Applies random linear projection to shrink those sub-vectors, which helps fill empty buckets and compress what's in them. This uses the Johnson-Lindenstrauss lemma to preserve important dot products while reducing size.
  - 𝗠𝘂𝗹𝘁𝗶𝗽𝗹𝗲 𝗥𝗲𝗽𝗲𝘁𝗶𝘁𝗶𝗼𝗻𝘀
    - Repeats steps 1-2 multiple times and concatenates results. This improves accuracy by capturing different aspects of the original embeddings.
  - 𝗙𝗶𝗻𝗮𝗹 𝗣𝗿𝗼𝗷𝗲𝗰𝘁𝗶𝗼𝗻
    - Creates the final single vector representation. The result? One fixed-size vector per document instead of hundreds.
- [Data Lakehouse vs. Data Lake vs. Data Warehouse](https://zilliz.com/glossary/data-lakehouse)
- [S3Vector](https://mp.weixin.qq.com/s/cB4dzM6IUZB5QW1IwrXBsA)
  - 动态局部更新索引（SPFresh）：写数据后只更一部分索引，不用全重建。 好处是写代价比较低，不需要重建索引，坏处是更新后recall会下降几个百分点；
  - 深度量化（4-bit PQ）：把高维向量压小，减少 S3 读写量 —— 好处是便宜、查得快，坏处依然是召回率低，Recall 稳定在 85% 左右，而且用户几乎无调参余地。
  - 后过滤（Post-Filter）机制 ：先粗略查一批，再按条件筛 —— 好处是好实现，能利用统一的底层索引结构，缺点是在过滤条件较多时，TopK 结果可能严重不足（我们测到删除 50% 数据后，TopK 20 只能返回 15 个结果）。说明S3团队用的基本就是开源索引，没有在索引侧做太多改造。
  - 分层缓存（Multi-tier Cache）：可能用 SSD  或者NVMe 做缓存，存最近查过的索引。新查询不命中SSD缓存时延迟明显较高
  - 大规模分布式调度 ：S3 本身有海量机器池，S3Vector 可能利用微服务将“读取-解压-检索”拆成流水线，让查询延迟分布非常稳。
- How to choose between Milvus, Elasticsearch and Pgvector
  - ## Milvus: **The first choice for vector databases**
    - Purpose-built for vectors
    - Superior performance at scale
    - Advanced indexing (IVF, HNSW)
    - Excellent horizontal scaling
    ### LIMITATIONS
    - Operational complexity
    - No relational data support
    ### BEST FOR
    - High-performance vector workloads at massive scale
  - ## Elasticsearch: **The preferred choice for traditional search**
    ### STRENGTHS
    - Advanced full-text search features (fuzzy matching, phrase queries)
    - Mature ecosystem & tooling
    - Rich aggregation capabilities
    ### LIMITATIONS
    - Vector performance lags
    - Resource-heavy stack
    - Complex query DSL
    ### BEST FOR
    - Applications need sophisticated search with aggregation capabilities.
  - ## Pgvector: **The PostgreSQL native**
    ### STRENGTHS
    - Familiar SQL interface
    - ACID compliance
    - Direct PostgreSQL integration
    - Unified data model
    ### LIMITATIONS
    - Performance ceiling
    - Limited indexing options
    - Traditional scaling issues
    - Not optimized for vectors
    ### BEST FOR
    - Adding vector capabilities to existing PostgreSQL applications
- [wal3: A Write-Ahead Log for Chroma, Built on Object Storage](https://trychroma.com/engineering/wal3)
  - 本质是“本地顺序写 + 后台异步复制到对象存储（S3 及其他






















