- [Agentic Design Patterns ](https://docs.google.com/document/d/1rsaK53T3Lg5KoGwvf8ukOUvbELRtH-V0LnOIFDxBryE/preview?tab=t.0)
- [Agents 2.0: From Shallow Loops to Deep Agents](https://www.philschmid.de/agents-2.0-deep-agents)
  - 显式规划（Explicit Planning）
    • 先用工具写出可持久化的 To-Do 列表／Markdown 计划，而不是在思维链里隐式规划。
    • 每一步执行后都会标记 pending / in_progress / done，并在失败时动态修改计划。
  - 分层委派（Hierarchical Delegation）
    - • 采用 Orchestrator → Sub-Agent 模式。
    - • 主控代理只负责拆任务、汇总结果；子代理（Researcher、Coder、Writer 等）各自带独立上下文执行多轮 while-loop，并返回精炼输出而非原始日志。
  - 持久化记忆（Persistent Memory）
    - • 关键状态写入外部文件系统或向量数据库，如 Claude Code、Manus 这类框架暴露 read/write 文件接口。
    - • 后续步骤按文件路径或向量检索“拉取”所需切片，实现“知道去哪里找”而非“记住全部文本”。
  - 极端上下文工程（Extreme Context Engineering）
    - • 通过冗长且结构化的系统提示，规定：何时先规划再行动、何时生成子代理、工具用法示例、文件命名规范、人-机协作协议等。简单一句“You are a helpful AI”无法触发深度行为。
- 智能体的崛起：“上下文”才是真正的护城河
  - AI 智能体的出现，标志着我们与 AI 协作的模式，从被动的“请求-响应”转变为主动的“指令-执行”。
    - 我们可以用自然语言下达一个宏观的目标，例如“重构用户认证模块以支持新的第三方登录”，然后由智能体自主地分析、定位、修改并验证相关代码。
  - 上下文工程，则是一种构建信息环境的科学，它更关注于为模型提供一个高质量、高信噪比的信息场。在这个信息场中，模型不再需要去“猜”，而是能够基于充分的依据去“推理”和“决策”。
  - 一个高质量的上下文环境是如何构建的呢？其核心在于建立一个能够深刻理解开发者“意图”的检索系统。
- [如何为 AI Agent设计有效上下文工程](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
  - 为什么需要上下文工程
    - 上下文窗口虽在扩大，但 LLM 仍会出现“context rot”——随着 token 数量增加，检索与长程推理准确率下降。
    - Transformer 对 n 个 token 需处理 n² 组注意力关系；再叠加训练语料长序列稀缺，模型在长上下文上天然性能递减。
    - 因此 token 是“注意力预算”的稀缺资源：必须策划最小、高信号的 token 集合来驱动期望行为
  - 有效上下文的组成
    - 系统提示
    - • 选择合适 “高度” ：避免写死复杂逻辑，也避免过于抽象；用简单直接的语言定义角色、目标、约束。
    - • 用区块 / 标签（XML、Markdown）分隔：如 ## Tool guidance、## Output format 等。
    - • 先用最小提示测试 → 根据失败模式迭代添加具体指令或示例。
    - 工具（Tools）
    - • 每个工具应自包含、语义明确、少重叠；输入参数描述性强。
    - • 维持“最小可行工具集”，否则代理和人类都难以选择正确工具。
    - 范例（Few-shot）
    - • 选多样、规范案例，而非塞满所有边缘情况；示例对 LLM 是高效信号
  - 长时程任务的三大技术
    - 压缩（Compaction）
    - • 当聊天或任务 token 接近上限时，总结并开启新窗口。
    - • 先保证高召回，再逐步提高压缩精度；常见“低垂果实”是清理旧的工具调用输出。
    - • Claude Developer Platform 已内置“工具结果清除”特性。
    - 结构化笔记 / 代理记忆（Agentic Memory）
    - • 代理定期写 NOTES.md / to-do list 存储到上下文外，后续按需检索。
    - • 适用于迭代开发、游戏通关等需跨会话保持状态的场景；Claude 平台已提供文件式记忆工具。
    - 子代理架构（Sub-agent Architecture）
    - • 主代理负责高层规划；多个子代理各自用干净窗口深度探索，通过 1-2k token 摘要向主代理汇报。
    - • 适用于需要并行搜索的大型研究与分析任务。
  - 实践要点
    - 始终把上下文当做有限资源：token 数≠信息量，高信噪比才关键。
    - 对系统提示、工具、范例、历史数据统一采用“最小高信号”原则。
    - 根据任务特征选择：
    - • 高频互动 → 首选压缩
    - • 有明显里程碑 → 加记忆
    - • 并行探索 → 多代理
- [Building agents with the Claude Agent SDK](https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk)
- [What Makes 5% of AI Agents Actually Work in Production](https://www.motivenotes.ai/p/what-makes-5-of-ai-agents-actually)
  - 现在大家都还在拼 prompt，只有少数人开始拼上下文结构。
    - prompt engineering 已经不是核心了，context engineering 才是下一阶段的主战场。给再聪明的大模型喂进去一堆乱七八糟的输入，它还是只会胡说八道。
    - 市面上跑得稳的 Agent，都是在“什么该让模型看、怎么看、以什么形式看”上下了大功夫的，这一点现在应该是共识了。
  - 记忆系统这事，光是能存起来远远不够 Memory Isn’t Just Storage, It’s Architectural
    - 很多公司的 memory，说得好听点是长期记忆，难听点就是个聊天记录仓库。
    - 真正落地的系统要分层记忆（用户级 / 团队级 / 系统级）。文章读完我感觉更多的篇 B 端，C 端要思考的是结合业务来做分层记忆，并且要能让人知道 它记住了什么，并且用户能自己改。否则就不是记忆是监控。
  - 不迷信单模型，这年头还不做 routing 的 agent 就别说自己做 infra 了。
    - 这篇文章提到多模型路由，说得很对。不可能所有请求都丢给 GPT5，成本和时延直接炸掉，表现也未必好。
    - 真正合理的系统，一定是： 快速反应的轻模型做分类和前处理、重模型做主任务、补一个模型做验证或追问。 一个 agent 后面绑定的一定是一个 LLM 团队。
    - For trivial queries → local model (no network call)
    - For structured queries → call DSL → SQL translator
    - For complex analysis → call OpenAI / Anthropic / Gemini
    - Fallback or verification → dual-model redundancy (judge + responder)
  - 可追溯/可控/可信，是企业愿意用 Agent 的底线
    - 很多人只想着怎么让 agent 能回答，但企业更关心：这句话是从哪里来的？有没有越权？出了错我怎么追责？ AI 要可治理。
  - 最被低估的一点：Agent ≠ Chatbot
    - 这篇文章最后说到的一点我非常赞同但还不够狠：如果还在用聊天当所有用户交互的方式，那agent 最多是个语音助理。
    - 真的 agent 应该是：先用语言调度任务，然后在页面上看到结构化结果，还能继续点选、调整、组合下一步。这部分很多公司现在在尝试了，交互上比之前全部自然语言高效了太多。
- [Context Engineering for AI Agents with LangChain and Manus](https://www.youtube.com/watch?v=6_BcCthVvb8)
  - 上下文工程对于 AI 智能体至关重要，可以对抗“上下文腐烂”并管理工具调用产生的上下文信息。 
    - 当智能体自主使用工具时，观察结果会累积，从而导致较长的上下文，进而降低 LLM 的性能。上下文工程优化了信息流，仅提供必要的数据。
  - 上下文缩减涉及基于上下文长度的可逆“压缩”和不可逆“摘要”。 
    - 压缩将可重构的信息（例如，文件路径而不是完整内容）外部化，而摘要则会浓缩数据，始终保留最近的交互以保持模型的一致性。
  - “分层行动空间”有效地卸载了工具，扩展了智能体功能，而不会过度占用 LLM 的直接上下文。 
    - 这涉及核心原子函数调用、沙箱实用程序（Shell 命令）以及用于包 / API 的 Python 脚本，从而允许广泛的、可组合的功能，同时保持 LLM 接口的简单性。
  - 多代理系统受益于简单任务的“通信模式”和复杂、依赖历史的任务的“共享内存模式”。 
    - 通信模式传递特定指令，而共享内存模式授予完全的上下文访问权限。为子智能体定义输出模式可确保结构化和可靠的信息交换。
  - https://drive.google.com/file/d/1QGJ-BrdiTGslS71sYH4OJoidsry3Ps9g/view
  - https://docs.google.com/presentation/d/16aaXLu40GugY-kOpqDU4e-S0hD1FmHcNyF0rRRnb1OU/edit?slide=id.p#slide=id.p
  - building production-ready AI applications through 6 core components:
   - 𝗔𝗴𝗲𝗻𝘁𝘀: The decision-making brain that orchestrates information flow and adapts strategies dynamically
   - 𝗤𝘂𝗲𝗿𝘆 𝗔𝘂𝗴𝗺𝗲𝗻𝘁𝗮𝘁𝗶𝗼𝗻: Techniques for transforming messy user requests into precise, machine-readable intent through rewriting, expansion, and decomposition
   - 𝗥𝗲𝘁𝗿𝗶𝗲𝘃𝗮𝗹: Strategies for chunking and retrieving the perfect piece of information from your knowledge base (semantic chunking, late chunking, hierarchical approaches)
   - 𝗣𝗿𝗼𝗺𝗽𝘁𝗶𝗻𝗴 𝗧𝗲𝗰𝗵𝗻𝗶𝗾𝘂𝗲𝘀: From Chain of Thought to ReAct frameworks - how to guide model reasoning effectively
   - 𝗠𝗲𝗺𝗼𝗿𝘆: Architecting short-term and long-term memory systems that give your application a sense of history and the ability to learn
   - 𝗧𝗼𝗼𝗹𝘀: Connecting LLMs to the outside world through function calling, the Model Context Protocol (MCP), and composable architectures
- Context Offload
  - 工具分成了 3 层：
    - 第 1 层：函数调用 (Function Calling) 这是最基础的一层，只保留一小组固定的、原子化的函数，比如：读写文件、执行 Shell 命令、搜索文件等。
    - 第 2 层：沙箱实用程序 (Sandbox Utilities) 这一层包含了一些更复杂的工具，Manus 在系统提示词里会直接告诉 LLM，在一个特定的文件夹里有很多预装的命令行工具。
    - 第 3 层：代码包与 API (Packages and APIs)  LLM 实时编写 Python 代码，通过代码实现更复杂的功能。比如用户想查询某个 API 的数据，可以直接用 Python 写一个函数，fetch API 的数据，并解析成需要的格式。
  - Manus 也是大量采用“智能体即工具 (agent as tool)”的模式。把子智能体当工具用，比如负责检索是一个子智能体，但是这个子智能体在主 Agent 看来就是一个工具
- [Claude 的 Agent Skills ](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/overview)
  - Skills let you package specialized knowledge into reusable capabilities that Claude loads on demand as agents tackle more complex tasks
  - 本质上是一种“上下文卸载”，把冗长的技能信息移出上下文，按需加载
  - 在全局或者项目目录下的 .claude/skills 下面添加目录，并且放一个包含meta信息的 SKILL\.md 文件，就可以引导 Agents 去学习使用这些 Skill。
  - https://github.com/anthropics/skills
- [Agent 的底层逻辑](https://mp.weixin.qq.com/s/tewBKHgbyrjxUjAOmkXI7A)
  - 核心立论
    - Agent 的质变不在于模型智力再次飞跃，而在于“思考→行动→观察”这一闭环流程带来的新型认知能力。
    - Agent = LLM + 结构化流程 + 工具接口 + 长/短期记忆管理。开发者的角色正在从 Prompt Engineer 转向 Agent 流程架构师
  - 能力演进五阶段
    - 原生天才：一次性回答＝普通 ChatGPT 调用。
    - 思考者：Chain of Thought（CoT）强制显式推理步骤。
    - 细心人：Reflexion 自我反思→复盘→下一轮改进。
    - 战略家：Planning 先拆解宏观任务、动态重排。
    - 学者：ReAct 将 Thought-Act-Observe 与外部工具深度耦合，突破训练语料边界。
  - 流程的三重技术价值
    - 结构化：CoT / ToT / Planning 为推理搭脚手架，对抗思维混沌。
      - 规划（Planning） 流程 在“宏观”层面，用结构对抗混沌
      - 思维链（Chain of Thought, CoT） 在“微观”层面，用结构对抗混沌
    - 迭代化：Reflexion / Summarization 把失败压缩成高信息密度“经验便签”，对抗上下文窗口限制。
      - 我们设计的反思（Reflection）、总结（Summarization）等流程，本质上是在为AI的记忆，量身打造了一套“高效的压缩算法”
    - 交互化：ReAct + ToolUse 让模型通过 API、数据库、代码执行等触手感知世界，对抗幻觉
  - 结论：重新定义上下文（Context）
    - 上下文不是需要我们去“填充”的目标，而是我们设计的卓越流程所“产出”的结果
    - 一个好的流程，天然就是一位“上下文架构师”。它通过两大核心机制来对抗遗忘和混乱：
      - 高效的信息压缩：精准的选择性注入
- [Master multi-agent system](https://drive.google.com/file/d/1WN7o417VOpvQWaV5nNFecEMYqOzEtZlc/view)
- AI 智能体在调用工具时遇到的三大难题：Token 成本、延迟 (latency) 和工具组合的效率
  - 把“代码执行”和“模型编写的代码” (MCP, Model-Written Code) 结合了起来。它不再让 AI 智能体直接去“调用工具”，而是把这些工具“伪装”成代码 API，让 AI 智能体像程序员一样通过写代码来使用它们。
  - 把这些“模型编写的代码” (MCP) 工具包打包成代码 API（比如 TypeScript 模块）。AI 智能体可以像程序员一样“导入” (import) 并通过编程来调用它们
  - 工具的“渐进式发现”：不再一股脑加载所有工具
    - AI 智能体学会了“按需取用”，通过搜索文件系统或调用 search_tools（搜索工具）函数，只在需要时才加载当前任务相关的工具定义
  - “数据本地处理”：在把结果喂给大语言模型 (LLM) 之前，先在代码执行环境里把数据处理好（比如筛选、转换、汇总）
  - 更优的控制流 (Control Flow)：与其让 AI 智能体一步步地“指挥”工具（比如“做完A，再做B”），不如直接用代码原生的循环 (loops)、条件判断 (conditionals) 和错误处理来管理流程。
  - 隐私保护：敏感数据可以在整个工作流中传递，而完全不进入大模型的“视野”（上下文）
  - 状态持久化 (State Persistence)：AI 智能体可以把中间结果存成文件，“断点续传”
  - 可复用的“技能包”：AI 智能体可以把写好的有效代码保存成“可复用函数”（并配上 SKILL .MD 文档）
- [Effective context engineering for AI agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
  - Context Engineering → 在整个推理生命周期内，持续策划进入 LLM 上下文窗口的所有 token：
    - 系统指令 + 工具定义 + MCP 协议 + 检索数据 + 消息历史 …
    - 目标：在模型固有限制下，找到最小且高信号的 token 集合，最大化期望行为
  - 为什么必须做上下文工程？
    - 上下文衰减（context-rot）：随着 token 数量增加，模型回忆上下文细节的准确率下降。
    - 注意力预算：Transformer 对 n 个 token 形成 n² 关系，长序列时注意力被“稀释”。
    - 位置插值等技术虽可扩长窗口，但会引入位置理解退化 → “性能梯度而非硬崖”。
    - 结论：上下文是一种递减边际收益的有限资源，需要精打细算。
  - 长时域任务三大技术
    ① 压缩 (Compaction)	当对话接近窗口上限时，总结后重启新窗口	优先召回 → 再压缩；先删除易冗余的“工具 I/O”
    ② 结构化笔记 (Structured Note-Taking / Agent Memory)	将笔记持久化到外部存储，后续按需检索	TODO 列表、NOTES.md、游戏坐标等；Claude Memory 工具已上线
    ③ 子智能体架构 (Sub-Agents)	用专门子智能体 + 主控智能体分治上下文	主智能体保持宏观计划，子智能体各自使用干净窗口
  -  黄金法则
    - 少即是多 📉不是上下文越多越好，而是相关信息越精准越好
    - 结构化胜于混乱 📋结构化的上下文比杂乱的信息堆砌更有效
    - 外部记忆解放内部上下文 💾使用外部存储保存长期信息
    - 专业化提升效率 🎯多智能体架构中的专业分工优于单一通用智能体
    - 动态管理优于静态配置 🔄根据任务阶段动态调整上下文内容
  - Future
    - 自动化上下文优化：AI 自主决定保留什么信息
    - 智能压缩算法：更高效的信息提炼
    - 记忆管理系统：类人的长短期记忆机制
    - 跨模态上下文：整合文本、图像、代码等多种形式
- Context Engineering: Sessions, Memory 白皮书中文
  -  LLM本质上是无状态的，其推理能力局限于单次API调用的上下文窗口。要构建有状态的智能代理，必须动态组装和管理信息——这就是上下文工程
  - 会话（Sessions）
    - 封装单次连续对话的即时对话历史和工作记忆的容器
    - 组成：事件（Events）+ 状态（State）
    - 事件类型：用户输入、代理响应、工具调用、工具输出
    - 长对话管理策略
      - 挑战：上下文窗口限制、API成本增加、延迟增加、质量下降
      - 压缩策略：
      - 1. 保留最后N轮：滑动窗口
      - 2. 基于令牌截断：从最近消息往回计数
      - 3. 递归摘要：用LLM摘要替换旧消息
  - 记忆（Memory） 跨多个会话捕获和整合关键信息的长期持久化机制
    | 维度   | RAG        | Memory     |
    |------|------------|------------|
    | 目标   | 注入外部事实知识   | 创建个性化有状态体验 |
    | 数据源  | 静态预索引外部知识库 | 用户与代理的对话   |
    | 隔离级别 | 通常共享       | 高度隔离（每用户）  |
    | 信息类型 | 静态权威事实     | 动态用户特定信息   |
    | 读取模式 | 几乎总是工具方式   | 工具方式或静态检索  |
- [When Software Disappears: The Real Beginning of AI-Native Systems](https://www.entropycontroltheory.com/p/when-software-disappears-the-real)
  - 一场“从点按钮到用语言表达意图”的认知迁移
- 长上下文能力在迅速提升（百万级 token），但“能放”不等于“多放就好”：信息冗余会带来干扰、成本上升与效果下降。
  - 长上下文在实际应用中有 4 种典型失败模式，需要通过“上下文工程（Context Engineering）”解决。
  - 在多种上下文管理策略中，**Context Pruning（上下文剪枝）**是 RAG 优化的核心环节：在检索后、生成前精准过滤无关内容。
  - Naver 的 Provence / XProvence 是专门为 Context Pruning 设计的轻量模型，可同时完成 重排序（rerank）+ 句子级剪枝，并支持多语言版本
- [Agent Design Is Still Hard](https://lucumr.pocoo.org/2025/11/21/agents-are-hard/)
  - LLM 本质还是一个API，除了做一层抽象把各家接口进行统一（openrouter），没有其他任何抽象的需要；
  - 缓存不太认同，隐式缓存无脑，也很好用；
  - think tool 是所有做 agent 都会用的高阶技巧，可以看看 anthropic 的 think tool 博客。我打算专门写一篇文章来讨论；
  - agent 搭配沙盒，用文件系统做上下文是一个非常好的偷懒办法；
  -  还有 hacker news 的一些讨论
    - 不应该是 sub-agent 而是 agent as a tool; sub-agent太容易误导人了；
    - 很多团队都没有 eval 上的基建，或者大家都没有意识到 ，Prompt 工程最重要的一部分就可以可观测，可验证，并且要快。
  - 选择哪个 Agent SDK：目前更倾向直接使用底层平台 SDK，而不是高层抽象 SDK
  - 缓存（Caching）：Anthropic 的「显式缓存点」虽然麻烦，但给予了极大的控制力和成本可预期性
  - 在 Agent 循环中的 Reinforcement（强化： 每次工具调用都是一次插入「额外信息/纠偏指令」的机会
  - 隔离失败（Isolate Failures）：避免大量失败日志和错误状态污染主上下文
  - 所有工具/子代理共享一个虚拟文件系统作为「公共状态」
  - agent 主循环并不直接「对话」，而是通过一个专门的 output tool 向人类输出
  - Anthropic Claude（Haiku/Sonnet）作为主 agent loop 的工具调用模型 + Gemini 系列做大文档/图像处理
- [10个上下文处理技巧](https://arxiv.org/html/2507.13334v1)
  - 长上下文 ≠ 简单增大 context window：需要从架构（SSM、Dilated/Linear Attention）、位置编码（RoPE/NTK 插值）、工程优化（FlashAttention、GQA、KV 削减）多层联动。
  - 自我精炼将成为“默认范式”：通过 Self-Refine / Reflexion / N-CRITICS 等框架，让模型在一次请求内完成多轮“审—改”会是常态。
- [Introducing advanced tool use on the Claude Developer Platform](https://www.anthropic.com/engineering/advanced-tool-use)
  - 在发 skills 的时候，针对工具膨胀浪费 token 提出了， Prompt 分层加载/复用，用代码执行&串联api/mcp（manus 把这个叫做上下文卸载）两个方法
  - 发 opus 的同时，把这两个方法固定到了推理 API 层面，Tool Search Tool，解决工具的发现&懒加载，Programmatic Tool Calling 实现代码执行工具。
  - 高级工具使用能力，提出了构建大规模 Agent 时的三大技术方向：
     - Tool Search Tool： 通过 defer_loading 和搜索式加载工具定义，大幅减小工具定义对上下文的占用，并提高工具选择准确率。
     - Programmatic Tool Calling： 通过让 Claude 写代码来 orchestrate 工具调用，把复杂工作流中的中间数据处理放在代码执行环境中执行，从而：
       - 减少 token 消耗和延迟；
       - 显式控制流程，提高复杂场景的可靠性和准确率。
     - Tool Use Examples： 在工具 schema 中补充多条真实使用示例，向模型传达格式规范、参数关联和使用模式，显著降低参数错误和误用。
  - 先识别当前 agent 的最大瓶颈：
    - 上下文被工具定义挤爆 → 优先上 Tool Search Tool。
    - 中间结果太多污染上下文 → 优先上 Programmatic Tool Calling。
    - 参数错误/调用错误多 → 优先上 Tool Use Examples。
  - Tool Search Tool：保证 找到正确的工具，且不拖垮上下文。
  - Programmatic Tool Calling：保证 高效执行和编排 工具调用。
  - Tool Use Examples：保证 调用参数和用法正确。
- [Effective harnesses for long-running agents](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents)
  - 问题背景：长时运行 Agent 的核心难题
    - 随着 AI 智能体承担跨越数小时或数天的复杂任务，如何让智能体在多个上下文窗口之间保持一致的进展仍然是一个开放性问题
  - 解决方案：双智能体框架系统 Anthropic 为 Claude Agent SDK 开发了受软件工程实践启发的两阶段智能体框架：
    - 1. Initializer Agent（初始化智能体）
      - 使用时机： 第一次会话
      - 功能： 使用专门的提示词要求模型设置初始环境
      - 创建的组件：
        - init.sh 脚本：环境初始化脚本
        - claude-progress.txt 文件：记录智能体所做工作的日志
        - 初始 git commit：显示添加了哪些文件的首次提交
    - 2. Coding Agent（编码智能体）
      - 使用时机： 每个后续会话
      - 核心任务：
        - 进行增量式进展（incremental progress）
        - 留下结构化更新（structured updates）
        - 在会话结束时将环境保持在干净状态











